{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: _Introduction to Spark SQL with Python_:\n",
    "1.  sql\n",
    "2.  window function\n",
    "3.  ui\n",
    "4.  nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _1. PySpark SQL_:\n",
    "-  ```spark.sql(...)```\n",
    "-  Window Functions:\n",
    "    -  ```pyspark.sql.Window```\n",
    "    -  ```pyspark.sql.Window.partitionBy```\n",
    "    -  ```pyspark.sql.Window.orderBy```\n",
    "    -  ```pyspark.sql.window.WindowSpec``` => ex: ```window = Window.partitionBy('col1').orderBy('col2')```\n",
    "    -  aggregate function for every row per group\n",
    "-  ```agg(...)``` => summarize one column at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sql table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"course_data/spark_sql/trainsched.txt\"\n",
    "df = spark.read.csv(PATH, header=True)\n",
    "df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+\n",
      "|train_id|      station| time|\n",
      "+--------+-------------+-----+\n",
      "|     324|San Francisco|7:59a|\n",
      "|     324|  22nd Street|8:03a|\n",
      "|     324|     Millbrae|8:16a|\n",
      "|     324|    Hillsdale|8:24a|\n",
      "|     324| Redwood City|8:31a|\n",
      "|     324|    Palo Alto|8:37a|\n",
      "|     324|     San Jose|9:05a|\n",
      "|     217|       Gilroy|6:06a|\n",
      "|     217|   San Martin|6:15a|\n",
      "|     217|  Morgan Hill|6:21a|\n",
      "|     217| Blossom Hill|6:36a|\n",
      "|     217|      Capitol|6:42a|\n",
      "|     217|       Tamien|6:50a|\n",
      "|     217|     San Jose|6:59a|\n",
      "+--------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"schedule\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### table cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|train_id|   string|   null|\n",
      "| station|   string|   null|\n",
      "|    time|   string|   null|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE schedule\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### window function sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([T.StructField('train_id', T.StringType()),\\\n",
    "                       T.StructField('station', T.StringType()),\\\n",
    "                       T.StructField('time', T.StringType()),\\\n",
    "                       T.StructField('diff_min', T.DoubleType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    ".createDataFrame(\\\n",
    " [Row(train_id='217', station='Gilroy', time='6:06a', diff_min=9.0),\n",
    "  Row(train_id='217', station='San Martin', time='6:15a', diff_min=6.0),\n",
    "  Row(train_id='217', station='Morgan Hill', time='6:21a', diff_min=15.0),\n",
    "  Row(train_id='217', station='Blossom Hill', time='6:36a', diff_min=6.0),\n",
    "  Row(train_id='217', station='Capitol', time='6:42a', diff_min=8.0),\n",
    "  Row(train_id='217', station='Tamien', time='6:50a', diff_min=9.0),\n",
    "  Row(train_id='217', station='San Jose', time='6:59a', diff_min=None),\n",
    "  Row(train_id='324', station='San Francisco', time='7:59a', diff_min=4.0),\n",
    "  Row(train_id='324', station='22nd Street', time='8:03a', diff_min=13.0),\n",
    "  Row(train_id='324', station='Millbrae', time='8:16a', diff_min=8.0),\n",
    "  Row(train_id='324', station='Hillsdale', time='8:24a', diff_min=7.0),\n",
    "  Row(train_id='324', station='Redwood City', time='8:31a', diff_min=6.0),\n",
    "  Row(train_id='324', station='Palo Alto', time='8:37a', diff_min=28.0),\n",
    "  Row(train_id='324', station='San Jose', time='9:05a', diff_min=None)],\\\n",
    "  schema=schema)\n",
    "\n",
    "df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+-------------+\n",
      "|train_id|      station| time|diff_min|running_total|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "|     217|       Gilroy|6:06a|     9.0|          9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|         15.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|         30.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|         36.0|\n",
      "|     217|      Capitol|6:42a|     8.0|         44.0|\n",
      "|     217|       Tamien|6:50a|     9.0|         53.0|\n",
      "|     217|     San Jose|6:59a|    null|         53.0|\n",
      "|     324|San Francisco|7:59a|     4.0|          4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|         17.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|         25.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|         32.0|\n",
      "|     324| Redwood City|8:31a|     6.0|         38.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|         66.0|\n",
      "|     324|     San Jose|9:05a|    null|         66.0|\n",
      "+--------+-------------+-----+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time, diff_min,\n",
    "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Run the query and display the result\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition by => dot notation vs sql notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-----+---------+\n",
      "|row|train_id|      station| time|time_next|\n",
      "+---+--------+-------------+-----+---------+\n",
      "|  1|     217|       Gilroy|6:06a|    6:15a|\n",
      "|  2|     217|   San Martin|6:15a|    6:21a|\n",
      "|  3|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|  4|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|  5|     217|      Capitol|6:42a|    6:50a|\n",
      "|  6|     217|       Tamien|6:50a|    6:59a|\n",
      "|  7|     217|     San Jose|6:59a|    7:59a|\n",
      "|  8|     324|San Francisco|7:59a|    8:03a|\n",
      "|  9|     324|  22nd Street|8:03a|    8:16a|\n",
      "| 10|     324|     Millbrae|8:16a|    8:24a|\n",
      "| 11|     324|    Hillsdale|8:24a|    8:31a|\n",
      "| 12|     324| Redwood City|8:31a|    8:37a|\n",
      "| 13|     324|    Palo Alto|8:37a|    9:05a|\n",
      "| 14|     324|     San Jose|9:05a|     null|\n",
      "+---+--------+-------------+-----+---------+\n",
      "\n",
      "==========\n",
      "+---+--------+-------------+-----+---------+\n",
      "|row|train_id|      station| time|time_next|\n",
      "+---+--------+-------------+-----+---------+\n",
      "|  1|     217|       Gilroy|6:06a|    6:15a|\n",
      "|  2|     217|   San Martin|6:15a|    6:21a|\n",
      "|  3|     217|  Morgan Hill|6:21a|    6:36a|\n",
      "|  4|     217| Blossom Hill|6:36a|    6:42a|\n",
      "|  5|     217|      Capitol|6:42a|    6:50a|\n",
      "|  6|     217|       Tamien|6:50a|    6:59a|\n",
      "|  7|     217|     San Jose|6:59a|     null|\n",
      "|  8|     324|San Francisco|7:59a|    8:03a|\n",
      "|  9|     324|  22nd Street|8:03a|    8:16a|\n",
      "| 10|     324|     Millbrae|8:16a|    8:24a|\n",
      "| 11|     324|    Hillsdale|8:24a|    8:31a|\n",
      "| 12|     324| Redwood City|8:31a|    8:37a|\n",
      "| 13|     324|    Palo Alto|8:37a|    9:05a|\n",
      "| 14|     324|     San Jose|9:05a|     null|\n",
      "+---+--------+-------------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(bad_query).show()\n",
    "\n",
    "# Give the number of the bad row\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause\n",
    "clause = \"PARTITION BY train_id\"\n",
    "\n",
    "print(\"=\"*10)\n",
    "\n",
    "good_query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(good_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregation => dot notation vs sql notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|train_id|start|\n",
      "+--------+-----+\n",
      "|     217|6:06a|\n",
      "|     324|7:59a|\n",
      "+--------+-----+\n",
      "\n",
      "==========\n",
      "\n",
      "\n",
      "+--------+---------+---------+\n",
      "|train_id|min(time)|max(time)|\n",
      "+--------+---------+---------+\n",
      "|     217|    6:06a|    6:59a|\n",
      "|     324|    7:59a|    9:05a|\n",
      "+--------+---------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|train_id|max(time)|\n",
      "+--------+---------+\n",
      "|     217|    6:59a|\n",
      "|     324|    9:05a|\n",
      "+--------+---------+\n",
      "\n",
      "max(time)\n"
     ]
    }
   ],
   "source": [
    "# Give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\n",
    "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\n",
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n",
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expr = [F.min(F.col(\"time\")).alias('start'), F.max(F.col(\"time\")).alias('end')]\n",
    "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "dot_df.show()\n",
    "\n",
    "# Write a SQL query giving a result identical to dot_df\n",
    "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+---------+\n",
      "|train_id|      station| time|diff_min|time_next|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "|     217|       Gilroy|6:06a|     9.0|    6:15a|\n",
      "|     217|   San Martin|6:15a|     6.0|    6:21a|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|    6:36a|\n",
      "|     217| Blossom Hill|6:36a|     6.0|    6:42a|\n",
      "|     217|      Capitol|6:42a|     8.0|    6:50a|\n",
      "|     217|       Tamien|6:50a|     9.0|    6:59a|\n",
      "|     217|     San Jose|6:59a|    null|     null|\n",
      "|     324|San Francisco|7:59a|     4.0|    8:03a|\n",
      "|     324|  22nd Street|8:03a|    13.0|    8:16a|\n",
      "|     324|     Millbrae|8:16a|     8.0|    8:24a|\n",
      "|     324|    Hillsdale|8:24a|     7.0|    8:31a|\n",
      "|     324| Redwood City|8:31a|     6.0|    8:37a|\n",
      "|     324|    Palo Alto|8:37a|    28.0|    9:05a|\n",
      "|     324|     San Jose|9:05a|    null|     null|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "\n",
      "+--------+-------------+-----+--------+---------+\n",
      "|train_id|      station| time|diff_min|time_next|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "|     217|       Gilroy|6:06a|     9.0|    6:15a|\n",
      "|     217|   San Martin|6:15a|     6.0|    6:21a|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|    6:36a|\n",
      "|     217| Blossom Hill|6:36a|     6.0|    6:42a|\n",
      "|     217|      Capitol|6:42a|     8.0|    6:50a|\n",
      "|     217|       Tamien|6:50a|     9.0|    6:59a|\n",
      "|     217|     San Jose|6:59a|    null|     null|\n",
      "|     324|San Francisco|7:59a|     4.0|    8:03a|\n",
      "|     324|  22nd Street|8:03a|    13.0|    8:16a|\n",
      "|     324|     Millbrae|8:16a|     8.0|    8:24a|\n",
      "|     324|    Hillsdale|8:24a|     7.0|    8:31a|\n",
      "|     324| Redwood City|8:31a|     6.0|    8:37a|\n",
      "|     324|    Palo Alto|8:37a|    28.0|    9:05a|\n",
      "|     324|     San Jose|9:05a|    null|     null|\n",
      "+--------+-------------+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT *, \n",
    "LEAD(time,1) OVER(PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Obtain the identical result using dot notation \n",
    "dot_df = df.withColumn('time_next', F.lead('time', 1)\n",
    "        .over(Window.partitionBy('train_id')\n",
    "        .orderBy('time')))\n",
    "\n",
    "dot_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### window function => dot notation vs sql notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"diff_min\", \"time_next\")\n",
    "df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+--------+\n",
      "|train_id|      station| time|diff_min|\n",
      "+--------+-------------+-----+--------+\n",
      "|     217|       Gilroy|6:06a|     9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|\n",
      "|     217|      Capitol|6:42a|     8.0|\n",
      "|     217|       Tamien|6:50a|     9.0|\n",
      "|     217|     San Jose|6:59a|    null|\n",
      "|     324|San Francisco|7:59a|     4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|\n",
      "|     324| Redwood City|8:31a|     6.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|\n",
      "|     324|     San Jose|9:05a|    null|\n",
      "+--------+-------------+-----+--------+\n",
      "\n",
      "+--------+-------------+-----+--------+\n",
      "|train_id|      station| time|diff_min|\n",
      "+--------+-------------+-----+--------+\n",
      "|     217|       Gilroy|6:06a|     9.0|\n",
      "|     217|   San Martin|6:15a|     6.0|\n",
      "|     217|  Morgan Hill|6:21a|    15.0|\n",
      "|     217| Blossom Hill|6:36a|     6.0|\n",
      "|     217|      Capitol|6:42a|     8.0|\n",
      "|     217|       Tamien|6:50a|     9.0|\n",
      "|     217|     San Jose|6:59a|    null|\n",
      "|     324|San Francisco|7:59a|     4.0|\n",
      "|     324|  22nd Street|8:03a|    13.0|\n",
      "|     324|     Millbrae|8:16a|     8.0|\n",
      "|     324|    Hillsdale|8:24a|     7.0|\n",
      "|     324| Redwood City|8:31a|     6.0|\n",
      "|     324|    Palo Alto|8:37a|    28.0|\n",
      "|     324|     San Jose|9:05a|    null|\n",
      "+--------+-------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window = Window.partitionBy('train_id').orderBy('time')\n",
    "dot_df = df.withColumn('diff_min', \n",
    "                    (F.unix_timestamp(F.lead('time', 1).over(window),'H:m') \n",
    "                     - F.unix_timestamp('time', 'H:m'))/60)\n",
    "\n",
    "dot_df.show()\n",
    "\n",
    "# Create a SQL query to obtain an identical result to dot_df \n",
    "query = \"\"\"\n",
    "SELECT *, \n",
    "(UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \n",
    " - UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min \n",
    "FROM schedule \n",
    "\"\"\"\n",
    "sql_df = spark.sql(query)\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _2. Using Window Function SQL for NLP_:\n",
    "-  ```split(...)```\n",
    "-  ```explode(...)```\n",
    "-  ```lag(...)``` => Window function: returns the value that is offset rows before the current row\n",
    "-  ```lead(...)``` => Window function: returns the value that is offset rows after the current row\n",
    "-  ```length(...)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|word  |id |\n",
      "+------+---+\n",
      "|it    |71 |\n",
      "|do    |72 |\n",
      "|not   |73 |\n",
      "|change|74 |\n",
      "|or    |75 |\n",
      "+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataframe\n",
    "df = spark.read.load('course_data/spark_sql/sherlock.parquet')\n",
    "\n",
    "# Filter and show the first 5 rows\n",
    "df.where('id > 70').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split / explode text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([T.StructField('clause', T.StringType()),\\\n",
    "                       T.StructField('id', T.LongType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clauses = spark\\\n",
    ".createDataFrame(\\\n",
    "[Row(clause='title', id=0),\n",
    " Row(clause='the adventures of sherlock holmes author', id=1),\n",
    " Row(clause='sir arthur conan doyle release date', id=2),\n",
    " Row(clause='march 1999', id=3),\n",
    " Row(clause='ebook 1661', id=4),\n",
    " Row(clause='most recently updated', id=5),\n",
    " Row(clause='november 29 2002', id=6),\n",
    " Row(clause='edition', id=7),\n",
    " Row(clause='12 language', id=8),\n",
    " Row(clause='english character set encoding', id=9),\n",
    " Row(clause='ascii', id=10),\n",
    " Row(clause='start of the project gutenberg ebook the adventures of sherlock holmes', id=11),\n",
    " Row(clause='additional editing by jose menendez', id=12),\n",
    " Row(clause='the adventures of sherlock holmes by sir arthur conan doyle contents i', id=13),\n",
    " Row(clause='a scandal in bohemia ii', id=14),\n",
    " Row(clause='the red', id=15),\n",
    " Row(clause='headed league iii', id=16),\n",
    " Row(clause='a case of identity iv', id=17),\n",
    " Row(clause='the boscombe valley mystery v', id=18),\n",
    " Row(clause='the five orange pips vi', id=19),\n",
    " Row(clause='the man with the twisted lip vii', id=20),\n",
    " Row(clause='the adventure of the blue carbuncle viii', id=21),\n",
    " Row(clause='the adventure of the speckled band ix', id=22),\n",
    " Row(clause='the adventure of the engineer s thumb x', id=23),\n",
    " Row(clause='the adventure of the noble bachelor xi', id=24),\n",
    " Row(clause='the adventure of the beryl coronet xii', id=25),\n",
    " Row(clause='the adventure of the copper beeches adventure i', id=26),\n",
    " Row(clause='a scandal in bohemia i', id=27),\n",
    " Row(clause='to sherlock holmes she is always the woman', id=28),\n",
    " Row(clause='i have seldom heard him mention her under any other name', id=29),\n",
    " Row(clause='in his eyes she eclipses and predominates the whole of her sex', id=30),\n",
    " Row(clause='it was not that he felt any emotion akin to love for irene adler', id=31),\n",
    " Row(clause='all emotions and that one particularly were abhorrent to his cold precise but admirably balanced mind', id=32),\n",
    " Row(clause='he was i take it the most perfect reasoning and observing machine that the world has seen but as a lover he would have placed himself in a false position', id=33),\n",
    " Row(clause='he never spoke of the softer passions save with a gibe and a sneer', id=34),\n",
    " Row(clause='they were admirable things for the observer', id=35),\n",
    " Row(clause='excellent for drawing the veil from men s motives and actions', id=36),\n",
    " Row(clause='but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results', id=37),\n",
    " Row(clause='grit in a sensitive instrument or a crack in one of his own high', id=38),\n",
    " Row(clause='power lenses would not be more disturbing than a strong emotion in a nature such as his', id=39),\n",
    " Row(clause='and yet there was but one woman to him and that woman was the late irene adler of dubious and questionable memory', id=40),\n",
    " Row(clause='i had seen little of holmes lately', id=41),\n",
    " Row(clause='my marriage had drifted us away from each other', id=42),\n",
    " Row(clause='my own complete happiness and the home', id=43),\n",
    " Row(clause='centred interests which rise up around the man who first finds himself master of his own establishment were sufficient to absorb all my attention while holmes who loathed every form of society with his whole bohemian soul remained in our lodgings in baker street buried among his old books and alternating from week to week between cocaine and ambition the drowsiness of the drug and the fierce energy of his own keen nature', id=44),\n",
    " Row(clause='he was still as ever deeply attracted by the study of crime and occupied his immense faculties and extraordinary powers of observation in following out those clues and clearing up those mysteries which had been abandoned as hopeless by the official police', id=45),\n",
    " Row(clause='from time to time i heard some vague account of his doings', id=46),\n",
    " Row(clause='of his summons to odessa in the case of the trepoff murder of his clearing up of the singular tragedy of the atkinson brothers at trincomalee and finally of the mission which he had accomplished so delicately and successfully for the reigning family of holland', id=47),\n",
    " Row(clause='beyond these signs of his activity however which i merely shared with all the readers of the daily press i knew little of my former friend and companion', id=48),\n",
    " Row(clause='one night', id=49),\n",
    " Row(clause='it was on the twentieth of march 1888', id=50),\n",
    " Row(clause='i was returning from a journey to a patient', id=51),\n",
    " Row(clause='for i had now returned to civil practice', id=52),\n",
    " Row(clause='when my way led me through baker street', id=53),\n",
    " Row(clause='as i passed the well', id=54),\n",
    " Row(clause='remembered door which must always be associated in my mind with my wooing and with the dark incidents of the study in scarlet i was seized with a keen desire to see holmes again and to know how he was employing his extraordinary powers', id=55),\n",
    " Row(clause='his rooms were brilliantly lit and even as i looked up i saw his tall spare figure pass twice in a dark silhouette against the blind', id=56),\n",
    " Row(clause='he was pacing the room swiftly eagerly with his head sunk upon his chest and his hands clasped behind him', id=57),\n",
    " Row(clause='to me who knew his every mood and habit his attitude and manner told their own story', id=58),\n",
    " Row(clause='he was at work again', id=59),\n",
    " Row(clause='he had risen out of his drug', id=60),\n",
    " Row(clause='created dreams and was hot upon the scent of some new problem', id=61),\n",
    " Row(clause='i rang the bell and was shown up to the chamber which had formerly been in part my own', id=62),\n",
    " Row(clause='his manner was not effusive', id=63),\n",
    " Row(clause='it seldom was', id=64),\n",
    " Row(clause='but he was glad i think to see me', id=65),\n",
    " Row(clause='with hardly a word spoken but with a kindly eye he waved me to an armchair threw across his case of cigars and indicated a spirit case and a gasogene in the corner', id=66),\n",
    " Row(clause='then he stood before the fire and looked me over in his singular introspective fashion', id=67),\n",
    " Row(clause='wedlock suits you he remarked', id=68),\n",
    " Row(clause='i think watson that you have put on seven and a half pounds since i saw you', id=69),\n",
    " Row(clause='seven', id=70),\n",
    " Row(clause='i answered', id=71),\n",
    " Row(clause='indeed i should have thought a little more', id=72),\n",
    " Row(clause='just a trifle more i fancy watson', id=73),\n",
    " Row(clause='and in practice again i observe', id=74),\n",
    " Row(clause='you did not tell me that you intended to go into harness', id=75),\n",
    " Row(clause='then how do you know', id=76),\n",
    " Row(clause='i see it i deduce it', id=77),\n",
    " Row(clause='how do i know that you have been getting yourself very wet lately and that you have a most clumsy and careless servant girl', id=78),\n",
    " Row(clause='my dear holmes said i this is too much', id=79),\n",
    " Row(clause='you would certainly have been burned had you lived a few centuries ago', id=80),\n",
    " Row(clause='it is true that i had a country walk on thursday and came home in a dreadful mess but as i have changed my clothes i canot imagine how you deduce it', id=81),\n",
    " Row(clause='as to mary jane she is incorrigible and my wife has given her notice but there again i fail to see how you work it out', id=82),\n",
    " Row(clause='he chuckled to himself and rubbed his long nervous hands together', id=83),\n",
    " Row(clause='it is simplicity itself said he', id=84),\n",
    " Row(clause='my eyes tell me that on the inside of your left shoe just where the firelight strikes it the leather is scored by six almost parallel cuts', id=85),\n",
    " Row(clause='obviously they have been caused by someone who has very carelessly scraped round the edges of the sole in order to remove crusted mud from it', id=86),\n",
    " Row(clause='hence you see my double deduction that you had been out in vile weather and that you had a particularly malignant boot', id=87),\n",
    " Row(clause='slitting specimen of the london slavey', id=88),\n",
    " Row(clause='as to your practice if a gentleman walks into my rooms smelling of iodoform with a black mark of nitrate of silver upon his right forefinger and a bulge on the right side of his top', id=89),\n",
    " Row(clause='hat to show where he has secreted his stethoscope i must be dull indeed if i do not pronounce him to be an active member of the medical profession', id=90),\n",
    " Row(clause='i could not help laughing at the ease with which he explained his process of deduction', id=91),\n",
    " Row(clause='when i hear you give your reasons i remarked the thing always appears to me to be so ridiculously simple that i could easily do it myself though at each successive instance of your reasoning i am baffled until you explain your process', id=92),\n",
    " Row(clause='and yet i believe that my eyes are as good as yours', id=93),\n",
    " Row(clause='quite so he answered lighting a cigarette and throwing himself down into an armchair', id=94),\n",
    " Row(clause='you see but you do not observe', id=95),\n",
    " Row(clause='the distinction is clear', id=96),\n",
    " Row(clause='for example you have frequently seen the steps which lead up from the hall to this room', id=97),\n",
    " Row(clause='frequently', id=98),\n",
    " Row(clause='how often', id=99)], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|words                                          |\n",
      "+-----------------------------------------------+\n",
      "|[title]                                        |\n",
      "|[the, adventures, of, sherlock, holmes, author]|\n",
      "|[sir, arthur, conan, doyle, release, date]     |\n",
      "|[march, 1999]                                  |\n",
      "|[ebook, 1661]                                  |\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|     title|\n",
      "|       the|\n",
      "|adventures|\n",
      "|        of|\n",
      "|  sherlock|\n",
      "|    holmes|\n",
      "|    author|\n",
      "|       sir|\n",
      "|    arthur|\n",
      "|     conan|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Number of rows:  1279\n"
     ]
    }
   ],
   "source": [
    "# Split the clause column into a column called words \n",
    "split_df = df_clauses.select(F.split('clause',' ').alias('words'))\n",
    "split_df.show(5, truncate=False)\n",
    "\n",
    "# Explode the words column into a column called word \n",
    "exploded_df = split_df.select(F.explode('words').alias('word'))\n",
    "exploded_df.show(10)\n",
    "\n",
    "# Count the resulting number of rows in exploded_df\n",
    "print(\"\\nNumber of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([T.StructField('word', T.StringType()),\\\n",
    "                       T.StructField('id', T.LongType()),\\\n",
    "                       T.StructField('part', T.IntegerType()),\\\n",
    "                       T.StructField('title', T.StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = spark\\\n",
    ".createDataFrame(\\\n",
    "[Row(word='scandal', id=305, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='in', id=306, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='bohemia', id=307, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='i', id=308, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='to', id=309, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='sherlock', id=310, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='holmes', id=311, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='she', id=312, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='is', id=313, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='red-headed', id=8861, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='league', id=8862, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='i', id=8863, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='had', id=8864, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='called', id=8865, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='upon', id=8866, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='my', id=8867, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='friend', id=8868, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='mr', id=8869, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='sherlock', id=8870, part=2, title='Sherlock Chapter II'),\n",
    " Row(word='case', id=18032, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='of', id=18033, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='identity', id=18034, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='my', id=18035, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='dear', id=18036, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='fellow', id=18037, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='said', id=18038, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='sherlock', id=18039, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='holmes', id=18040, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='as', id=18041, part=3, title='Sherlock Chapter III'),\n",
    " Row(word='always', id=314, part=1, title='Sherlock Chapter I'),\n",
    " Row(word='ix', id=68993, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='the', id=68994, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='adventure', id=68995, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='of', id=68996, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='the', id=68997, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='engineer', id=68998, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='s', id=68999, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='thumb', id=69000, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='of', id=69001, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='all', id=69002, part=9, title='Sherlock Chapter IX'),\n",
    " Row(word='x', id=77313, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='the', id=77314, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='adventure', id=77315, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='of', id=77316, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='the', id=77317, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='noble', id=77318, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='bachelor', id=77319, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='the', id=77320, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='lord', id=77321, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='st', id=77322, part=10, title='Sherlock Chapter X'),\n",
    " Row(word='xi', id=85462, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='the', id=85463, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='adventure', id=85464, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='of', id=85465, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='the', id=85466, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='beryl', id=85467, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='coronet', id=85468, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='holmes', id=85469, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='said', id=85470, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='i', id=85471, part=11, title='Sherlock Chapter XI'),\n",
    " Row(word='xii', id=95166, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='the', id=95167, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='adventure', id=95168, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='of', id=95169, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='the', id=95170, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='copper', id=95171, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='beeches', id=95172, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='to', id=95173, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='the', id=95174, part=12, title='Sherlock Chapter XII'),\n",
    " Row(word='man', id=95175, part=12, title='Sherlock Chapter XII')], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.createOrReplaceTempView(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+---------+---------+---------+\n",
      "|part|       w1|       w2|       w3|       w4|       w5|\n",
      "+----+---------+---------+---------+---------+---------+\n",
      "|  12|     null|     null|      xii|      the|adventure|\n",
      "|  12|     null|      xii|      the|adventure|       of|\n",
      "|  12|      xii|      the|adventure|       of|      the|\n",
      "|  12|      the|adventure|       of|      the|   copper|\n",
      "|  12|adventure|       of|      the|   copper|  beeches|\n",
      "|  12|       of|      the|   copper|  beeches|       to|\n",
      "|  12|      the|   copper|  beeches|       to|      the|\n",
      "|  12|   copper|  beeches|       to|      the|      man|\n",
      "|  12|  beeches|       to|      the|      man|     null|\n",
      "|  12|       to|      the|      man|     null|     null|\n",
      "+----+---------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "part,\n",
    "LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\n",
    "LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\n",
    "word AS w3,\n",
    "LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\n",
    "LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\n",
    "FROM text\n",
    "\"\"\"\n",
    "spark.sql(query).where(\"part = 12\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|title               |\n",
      "+--------------------+\n",
      "|Sherlock Chapter I  |\n",
      "|Sherlock Chapter II |\n",
      "|Sherlock Chapter III|\n",
      "|Sherlock Chapter IX |\n",
      "|Sherlock Chapter X  |\n",
      "|Sherlock Chapter XI |\n",
      "|Sherlock Chapter XII|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select('title')\\\n",
    "       .distinct()\\\n",
    "       .orderBy('title')\\\n",
    "       .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_part = df_text.coalesce(1) # make 1 partition for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text_part.rdd.getNumPartitions() # check # of partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe ```text_df_part``` is currently in a single partition. \n",
    "Suppose that you know that the upcoming processing steps are going to be grouping the data on titles. \n",
    "Processing the data will be most efficient if each title stays within a single machine. \n",
    "To avoid unnecessary shuffling of the data from one machine to another, let's repartition the dataframe into one partition per title, using the ```repartition``` and ```getNumPartitions``` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repartition text_df into 7 partitions on 'title' column\n",
    "df_repart = df_text_part.repartition(7, 'title')\n",
    "\n",
    "# Prove that repart_df has 7 partitions\n",
    "df_repart.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common word sequences =>  3-tuples in the text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------+-----+\n",
      "|        w1|       w2|    w3|count|\n",
      "+----------+---------+------+-----+\n",
      "| adventure|       of|   the|    4|\n",
      "|       the|adventure|    of|    4|\n",
      "|       the|     lord|    st|    1|\n",
      "|       the|      man|  null|    1|\n",
      "|  bachelor|      the|  lord|    1|\n",
      "|red-headed|   league|     i|    1|\n",
      "|    always|     null|  null|    1|\n",
      "|       she|       is|always|    1|\n",
      "|    called|     upon|    my|    1|\n",
      "|      said| sherlock|holmes|    1|\n",
      "+----------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 sequences of three words\n",
    "query = \"\"\"\n",
    "SELECT w1, w2, w3, COUNT(*) AS count FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3\n",
    "FROM text\n",
    ")\n",
    "GROUP BY w1, w2, w3\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "\"\"\" \n",
    "df = spark.sql(query)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common word sequences =>  sorted alpha desc order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+\n",
      "|   w1|      w2|       w3|\n",
      "+-----+--------+---------+\n",
      "|  xii|     the|adventure|\n",
      "|   xi|     the|adventure|\n",
      "|    x|     the|adventure|\n",
      "| upon|      my|   friend|\n",
      "|   to|     the|      man|\n",
      "|   to|sherlock|   holmes|\n",
      "|thumb|      of|      all|\n",
      "|  the|   noble| bachelor|\n",
      "|  the|     man|     null|\n",
      "|  the|    lord|       st|\n",
      "+-----+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unique 3-tuples sorted in descending order\n",
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT w1, w2, w3 FROM (\n",
    "   SELECT word AS w1,\n",
    "   LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "   LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3\n",
    "   FROM text\n",
    ")\n",
    "ORDER BY w1 DESC, w2 DESC, w3 DESC\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common word sequences =>  frequent 3-tuples per chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subquery = \"\"\"\n",
    "SELECT title, w1, w2, w3, COUNT(*) as count\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "    title,\n",
    "    word AS w1,\n",
    "    LEAD(word, 1) OVER(PARTITION BY title ORDER BY id ) AS w2,\n",
    "    LEAD(word, 2) OVER(PARTITION BY title ORDER BY id ) AS w3\n",
    "    FROM text\n",
    ")\n",
    "GROUP BY title, w1, w2, w3\n",
    "ORDER BY title, count DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------+--------+-----+\n",
      "|              title|        w1|      w2|      w3|count|\n",
      "+-------------------+----------+--------+--------+-----+\n",
      "| Sherlock Chapter I|    always|    null|    null|    1|\n",
      "| Sherlock Chapter I|   bohemia|       i|      to|    1|\n",
      "| Sherlock Chapter I|  sherlock|  holmes|     she|    1|\n",
      "| Sherlock Chapter I|        is|  always|    null|    1|\n",
      "| Sherlock Chapter I|       she|      is|  always|    1|\n",
      "| Sherlock Chapter I|    holmes|     she|      is|    1|\n",
      "| Sherlock Chapter I|   scandal|      in| bohemia|    1|\n",
      "| Sherlock Chapter I|        to|sherlock|  holmes|    1|\n",
      "| Sherlock Chapter I|         i|      to|sherlock|    1|\n",
      "| Sherlock Chapter I|        in| bohemia|       i|    1|\n",
      "|Sherlock Chapter II|      upon|      my|  friend|    1|\n",
      "|Sherlock Chapter II|        mr|sherlock|    null|    1|\n",
      "|Sherlock Chapter II|       had|  called|    upon|    1|\n",
      "|Sherlock Chapter II|    called|    upon|      my|    1|\n",
      "|Sherlock Chapter II|        my|  friend|      mr|    1|\n",
      "|Sherlock Chapter II|    friend|      mr|sherlock|    1|\n",
      "|Sherlock Chapter II|         i|     had|  called|    1|\n",
      "|Sherlock Chapter II|red-headed|  league|       i|    1|\n",
      "|Sherlock Chapter II|    league|       i|     had|    1|\n",
      "|Sherlock Chapter II|  sherlock|    null|    null|    1|\n",
      "+-------------------+----------+--------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(subquery).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+---------+-----+\n",
      "|               title|        w1|    w2|       w3|count|\n",
      "+--------------------+----------+------+---------+-----+\n",
      "|  Sherlock Chapter I|   scandal|    in|  bohemia|    1|\n",
      "| Sherlock Chapter II|red-headed|league|        i|    1|\n",
      "|Sherlock Chapter III|      case|    of| identity|    1|\n",
      "| Sherlock Chapter IX|        ix|   the|adventure|    1|\n",
      "|  Sherlock Chapter X|         x|   the|adventure|    1|\n",
      "| Sherlock Chapter XI|        xi|   the|adventure|    1|\n",
      "|Sherlock Chapter XII|       xii|   the|adventure|    1|\n",
      "+--------------------+----------+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   Most frequent 3-tuple per chapter\n",
    "query = \"\"\"\n",
    "SELECT title, w1, w2, w3, count FROM\n",
    "(\n",
    "  SELECT\n",
    "  title,\n",
    "  ROW_NUMBER() OVER (PARTITION BY title ORDER BY count DESC) AS row,\n",
    "  w1, w2, w3, count\n",
    "  FROM ( %s )\n",
    ")\n",
    "WHERE row = 1\n",
    "ORDER BY title ASC\n",
    "\"\"\" % subquery\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _3. Caching, Logging, and the Spark UI_:\n",
    "-  Caching:\n",
    "    -  keeps data in memory\n",
    "    -  lazy operation\n",
    "    -  only cachne if more than one operation needs to be performed on object\n",
    "    -  unpersist when object is no longer needed\n",
    "    -  ```cache()``` => cache DF\n",
    "    -  ```persist()``` => caches DF with ability customizing storage level\n",
    "    -  ```is_cached```\n",
    "    -  ```storageLevel``` ... DF defaults:\n",
    "        -  useDisk = True\n",
    "        -  useMemory = True\n",
    "        -  useOffHeap = False\n",
    "        -  deserialized = True\n",
    "        -  replication = 1\n",
    "    -  ```unpersist()``` => uncache DF\n",
    "    -  ```spark.catalog.cacheTable('...')```\n",
    "    -  ```spark.catalog.isCached(tableName = '...')```\n",
    "    -  ```spark.catalog.uncacheTable('...')```\n",
    "    -  ```spark.catalog.clearCache()```\n",
    "-  Spark UI:\n",
    "    -  runs on the driver host\n",
    "    -  ```Spark Task``` => unit of execution aka **partition** running on a single cpu core aka **thread slot**\n",
    "    -  ```Spark Stage``` => group of tasks that perform same computation in parallel\n",
    "    -  ```Spark Job``` => computation trigged by **action** sliced into one or more **stages**\n",
    "-  Logging\n",
    "-  Query Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def prep(df1, df2):\n",
    "    global begin\n",
    "    df1.unpersist()\n",
    "    df2.unpersist()\n",
    "    begin = time.time()\n",
    "\n",
    "def print_elapsed():\n",
    "    print(\"Overall elapsed : %.1f\" % (time.time() - begin))\n",
    "\n",
    "def run(df, name, elapsed=False):\n",
    "    start=time.time()\n",
    "    df.count()\n",
    "    print(\"%s : %.1fs\" % (name, (time.time()-start)))\n",
    "    if elapsed:\n",
    "        print_elapsed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_1st : 0.7s\n",
      "df1_2nd : 0.1s\n",
      "df2_1st : 0.1s\n",
      "df2_2nd : 0.0s\n",
      "Overall elapsed : 0.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(1000000)\n",
    "df2 = spark.range(1000000)\n",
    "\n",
    "# Unpersists df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Cache df1\n",
    "df1.cache()\n",
    "\n",
    "# Run actions on both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)\n",
    "\n",
    "# Prove df1 is cached\n",
    "print(df1.is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_1st : 0.3s\n",
      "df1_2nd : 0.1s\n",
      "df2_1st : 0.0s\n",
      "df2_2nd : 0.0s\n",
      "Overall elapsed : 0.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Unpersist df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Persist df2 using memory and disk storage level \n",
    "df2.persist(storageLevel=StorageLevel(True, True, False, False, 1)) # MEMORY_AND_DISK\n",
    "\n",
    "# Run actions both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### uncaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n",
      " [Table(name='schedule', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='text', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
      "table1 is cached:  True\n",
      "table1 is cached:  False\n"
     ]
    }
   ],
   "source": [
    "# List the tables\n",
    "print(\"Tables:\\n\", spark.catalog.listTables())\n",
    "\n",
    "# Cache table1 and Confirm that it is cached\n",
    "spark.catalog.cacheTable('text')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('text'))\n",
    "\n",
    "# Uncache table1 and confirm that it is uncached\n",
    "spark.catalog.uncacheTable('text')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logging:\n",
    "-  DEBUG - debug message\n",
    "-  INFO - info message\n",
    "-  WARNING - warn message\n",
    "-  ERROR - error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "#import sys\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.DEBUG,\n",
    "#                    format='%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Log columns of text_df as debug message\\nlogging.debug(\"text_df columns: %s\", text_df.columns)\\n\\n# Log whether table1 is cached as info message\\nlogging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\\n\\n# Log first row of text_df as warning message\\nlogging.warning(\"The first row of text_df:\\n %s\", text_df.first())\\n\\n# Log selected columns of text_df as error message\\nlogging.error(\"Selected columns: %s\", text_df.select(\"id\", \"word\"))\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Log columns of text_df as debug message\n",
    "logging.debug(\"text_df columns: %s\", text_df.columns)\n",
    "\n",
    "# Log whether table1 is cached as info message\n",
    "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\n",
    "\n",
    "# Log first row of text_df as warning message\n",
    "logging.warning(\"The first row of text_df:\\n %s\", text_df.first())\n",
    "\n",
    "# Log selected columns of text_df as error message\n",
    "logging.error(\"Selected columns: %s\", text_df.select(\"id\", \"word\"))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Uncomment the statements that do NOT trigger text_df\\nlogging.debug(\"text_df columns: %s\", text_df.columns)\\nlogging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\\n# logging.warning(\"The first row of text_df: %s\", text_df.first())\\nlogging.error(\"Selected columns: %s\", text_df.select(\"id\", \"word\"))\\nlogging.info(\"Tables: %s\", spark.sql(\"SHOW tables\").collect())\\nlogging.debug(\"First row: %s\", spark.sql(\"SELECT * FROM table1 LIMIT 1\"))\\n# logging.debug(\"Count: %s\", spark.sql(\"SELECT COUNT(*) AS count FROM table1\").collect())\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Uncomment the statements that do NOT trigger text_df\n",
    "logging.debug(\"text_df columns: %s\", text_df.columns)\n",
    "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\n",
    "# logging.warning(\"The first row of text_df: %s\", text_df.first())\n",
    "logging.error(\"Selected columns: %s\", text_df.select(\"id\", \"word\"))\n",
    "logging.info(\"Tables: %s\", spark.sql(\"SHOW tables\").collect())\n",
    "logging.debug(\"First row: %s\", spark.sql(\"SELECT * FROM table1 LIMIT 1\"))\n",
    "# logging.debug(\"Count: %s\", spark.sql(\"SELECT COUNT(*) AS count FROM table1\").collect())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explain plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[word#563,id#564L,part#565,title#566]\n",
      "====================================================================================================\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- *(1) Project\n",
      "         +- Scan ExistingRDD[word#563,id#564L,part#565,title#566]\n",
      "====================================================================================================\n",
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[], functions=[count(distinct word#563)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(2) HashAggregate(keys=[], functions=[partial_count(distinct word#563)])\n",
      "      +- *(2) HashAggregate(keys=[word#563], functions=[])\n",
      "         +- Exchange hashpartitioning(word#563, 200)\n",
      "            +- *(1) HashAggregate(keys=[word#563], functions=[])\n",
      "               +- *(1) Project [word#563]\n",
      "                  +- Scan ExistingRDD[word#563,id#564L,part#565,title#566]\n"
     ]
    }
   ],
   "source": [
    "# Run explain on text_df\n",
    "df_text.explain()\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Run explain on \"SELECT COUNT(*) AS count\" query on table1\n",
    "spark.sql(\"SELECT COUNT(*) AS count FROM text\").explain()\n",
    "\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Run explain on \"COUNT(DISTINCT word) AS words\"\n",
    "spark.sql(\"SELECT COUNT(DISTINCT word) AS words FROM text\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\npart2_df.explain()\\n\\n== Physical Plan ==\\n*(1) Project [word#0, id#1L, part#2, title#3]\\n+- *(1) Filter (isnotnull(part#2) && (part#2 = 2))\\n   +- *(1) FileScan parquet [word#0,id#1L,part#2,title#3] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tmptef3j2qh/sherlock_parts.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(part), EqualTo(part,2)], ReadSchema: struct<word:string,id:bigint,part:int,title:string>\\n\\npart3_df.explain()\\n\\n== Physical Plan ==\\nInMemoryTableScan [word#9, id#10L, part#11, title#12]\\n   +- InMemoryRelation [word#9, id#10L, part#11, title#12], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\\n         +- *(1) Project [word#9, id#10L, part#11, title#12]\\n            +- *(1) Filter (isnotnull(part#11) && (part#11 = 4))\\n               +- *(1) FileScan parquet [word#9,id#10L,part#11,title#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tmptef3j2qh/sherlock_parts.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(part), EqualTo(part,4)], ReadSchema: struct<word:string,id:bigint,part:int,title:string>\\n               \\npart4_df.explain()\\n\\n== Physical Plan ==\\n*(1) FileScan parquet [word#38,id#39L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tmptef3j2qh/sherlock.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<word:string,id:bigint>\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "part2_df.explain()\n",
    "\n",
    "== Physical Plan ==\n",
    "*(1) Project [word#0, id#1L, part#2, title#3]\n",
    "+- *(1) Filter (isnotnull(part#2) && (part#2 = 2))\n",
    "   +- *(1) FileScan parquet [word#0,id#1L,part#2,title#3] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tmptef3j2qh/sherlock_parts.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(part), EqualTo(part,2)], ReadSchema: struct<word:string,id:bigint,part:int,title:string>\n",
    "\n",
    "part3_df.explain()\n",
    "\n",
    "== Physical Plan ==\n",
    "InMemoryTableScan [word#9, id#10L, part#11, title#12]\n",
    "   +- InMemoryRelation [word#9, id#10L, part#11, title#12], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
    "         +- *(1) Project [word#9, id#10L, part#11, title#12]\n",
    "            +- *(1) Filter (isnotnull(part#11) && (part#11 = 4))\n",
    "               +- *(1) FileScan parquet [word#9,id#10L,part#11,title#12] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tmptef3j2qh/sherlock_parts.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(part), EqualTo(part,4)], ReadSchema: struct<word:string,id:bigint,part:int,title:string>\n",
    "               \n",
    "part4_df.explain()\n",
    "\n",
    "== Physical Plan ==\n",
    "*(1) FileScan parquet [word#38,id#39L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tmptef3j2qh/sherlock.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<word:string,id:bigint>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The filename that part2_df was loaded from\n",
    "answer1 = 'sherlock_parts.parquet'\n",
    "\n",
    "# The (integer) part is loaded in part3_df\n",
    "answer2 = 4\n",
    "\n",
    "# The filename that part4_df loaded from\n",
    "answer3 = 'sherlock.parquet'\n",
    "\n",
    "# The value of the ReadSchema property in part4_df.explain() \n",
    "answer4 = 'struct<word:string,id:bigint>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _4. Text Classification_:\n",
    "-  ETS:\n",
    "    -  Extract\n",
    "    -  Transform\n",
    "    -  Selection\n",
    "-  Train/Test Split\n",
    "-  Evaluate (Binary Classification):\n",
    "    -  Probability Vector => [False %, True %]\n",
    "    -  AUC => classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([T.StructField('doc', T.ArrayType(T.StringType())),\\\n",
    "                       T.StructField('in_', T.ArrayType(T.StringType())),\\\n",
    "                       T.StructField('out', T.ArrayType(T.StringType()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = spark\\\n",
    ".createDataFrame(\\\n",
    "[Row(doc=['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'sir', 'arthur', 'conan', 'doyle'], in_=['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'sir', 'arthur', 'conan'], out=['doyle']),\n",
    " Row(doc=['15', 'in_', 'our', 'series', 'by', 'sir', 'arthur', 'conan', 'doyle'], in_=['15', 'in_', 'our', 'series', 'by', 'sir', 'arthur', 'conan'], out=['doyle']),\n",
    " Row(doc=['copyright', 'laws', 'are', 'changin_g', 'all', 'over', 'the', 'world'], in_=['copyright', 'laws', 'are', 'changin_g', 'all', 'over', 'the'], out=['world']),\n",
    " Row(doc=['be', 'sure', 'to', 'check', 'the', 'copyright', 'laws', 'for', 'your', 'country', 'before', 'downloadin_g', 'or', 'redistributin_g', 'this', 'or', 'any', 'other', 'project', 'gutenberg', 'ebook'], in_=['be', 'sure', 'to', 'check', 'the', 'copyright', 'laws', 'for', 'your', 'country', 'before', 'downloadin_g', 'or', 'redistributin_g', 'this', 'or', 'any', 'other', 'project', 'gutenberg'], out=['ebook']),\n",
    " Row(doc=['this', 'header', 'should', 'be', 'the', 'first', 'thin_g', 'seen', 'when', 'viewin_g', 'this', 'project', 'gutenberg', 'file'], in_=['this', 'header', 'should', 'be', 'the', 'first', 'thin_g', 'seen', 'when', 'viewin_g', 'this', 'project', 'gutenberg'], out=['file']),\n",
    " Row(doc=['please', 'do', 'not', 'remove', 'it'], in_=['please', 'do', 'not', 'remove'], out=['it']),\n",
    " Row(doc=['she', 'left', 'this', 'mornin_g', 'with', 'her', 'husband', 'by', 'the', '5'], in_=['by', 'this', 'with', 'left', 'mornin_g', 'the', 'she', 'husband', 'her'], out=['5']),\n",
    " Row(doc=['he', 'had', 'no', 'occupation', 'but', 'was', 'in_terested', 'in_', 'several', 'companies', 'and', 'went', 'in_to', 'town', 'as', 'a', 'rule', 'in_', 'the', 'mornin_g', 'returnin_g', 'by', 'the', '5'], in_=['by', 'was', 'companies', 'he', 'in_terested', 'as', 'a', 'returnin_g', 'and', 'occupation', 'but', 'mornin_g', 'in_', 'the', 'rule', 'had', 'several', 'town', 'no', 'went', 'in_to'], out=['5']),\n",
    " Row(doc=['5', 'vols'], in_=[], out=['vols']),\n",
    " Row(doc=['on', 'the', 'night', 'of', 'march', '5', '1770', 'a', 'crowd', 'on', 'the', 'streets', 'of', 'boston', 'began', 'to', 'jostle', 'and', 'tease', 'some', 'british', 'regulars', 'stationed', 'in_', 'the', 'town'], in_=['stationed', 'crowd', 'on', 'streets', 'night', 'march', 'a', '1770', 'boston', 'to', 'british', 'in_', 'the', 'tease', 'of', 'jostle', 'regulars', 'and', 'began', 'some'], out=['town']),\n",
    " Row(doc=['by', 'hurried', 'and', 'irregular', 'methods', 'delegates', 'were', 'elected', 'durin_g', 'the', 'summer', 'and', 'on', 'september', '5', 'the', 'congress', 'duly', 'assembled', 'in_', 'carpenter', 's', 'hall', 'in_', 'philadelphia'], in_=['carpenter', 'by', 'congress', 'were', 'hurried', 'delegates', 'elected', 'september', 'methods', 'irregular', 'in_', 'the', 'on', 'durin_g', 'duly', 'assembled', 'summer', 'and', 'hall'], out=['philadelphia']),\n",
    " Row(doc=['do', 'not', 'change', 'or', 'edit', 'the', 'header', 'without', 'written', 'permission'], in_=['do', 'not', 'change', 'or', 'edit', 'the', 'header', 'without', 'written'], out=['permission']),\n",
    " Row(doc=['please', 'read', 'the', 'legal', 'small', 'prin_t', 'and', 'other', 'in_formation', 'about', 'the', 'ebook', 'and', 'project', 'gutenberg', 'at', 'the', 'bottom', 'of', 'this', 'file'], in_=['please', 'read', 'the', 'legal', 'small', 'prin_t', 'and', 'other', 'in_formation', 'about', 'the', 'ebook', 'and', 'project', 'gutenberg', 'at', 'the', 'bottom', 'of', 'this'], out=['file']),\n",
    " Row(doc=['in_cluded', 'is', 'important', 'in_formation', 'about', 'your', 'specific', 'rights', 'and', 'restrictions', 'in_', 'how', 'the', 'file', 'may', 'be', 'used'], in_=['in_cluded', 'is', 'important', 'in_formation', 'about', 'your', 'specific', 'rights', 'and', 'restrictions', 'in_', 'how', 'the', 'file', 'may', 'be'], out=['used']),\n",
    " Row(doc=['you', 'can', 'also', 'fin_d', 'out', 'about', 'how', 'to', 'make', 'a', 'donation', 'to', 'project', 'gutenberg', 'and', 'how', 'to', 'get', 'in_volved'], in_=['you', 'can', 'also', 'fin_d', 'out', 'about', 'how', 'to', 'make', 'a', 'donation', 'to', 'project', 'gutenberg', 'and', 'how', 'to', 'get'], out=['in_volved'])],\\\n",
    " schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|                 doc|                 in_|    out|\n",
      "+--------------------+--------------------+-------+\n",
      "|[the, project, gu...|[the, project, gu...|[doyle]|\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_docs.where(F.array_contains('doc','sherlock')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns true if the value is a nonempty vector\n",
    "nonempty_udf = F.udf(lambda x:  \n",
    "    True if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "    else False, T.BooleanType())\n",
    "\n",
    "# Returns first element of the array as string\n",
    "s_udf = F.udf(lambda x: str(x[0]) if (x and type(x) is list and len(x) > 0)\n",
    "    else '', T.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIVIAL_TOKENS = {'','0','1','2','3','4','5','6','7','8','9','b','c','e',\n",
    "                  'f','g','h','j','k','l','m','n','o','p','pp','q','r',\n",
    "                  's','t','u','v','w','x','y','z'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+\n",
      "|                 doc|                 in_|           out|\n",
      "+--------------------+--------------------+--------------+\n",
      "|[she, left, this,...|[by, this, with, ...|           [5]|\n",
      "|[he, had, no, occ...|[by, was, compani...|           [5]|\n",
      "|           [5, vols]|                  []|        [vols]|\n",
      "|[on, the, night, ...|[stationed, crowd...|        [town]|\n",
      "|[by, hurried, and...|[carpenter, by, c...|[philadelphia]|\n",
      "+--------------------+--------------------+--------------+\n",
      "\n",
      "+--------------------+--------------------+--------------+\n",
      "|                 doc|                 in_|           out|\n",
      "+--------------------+--------------------+--------------+\n",
      "|[she, left, this,...|[by, this, with, ...|            []|\n",
      "|[he, had, no, occ...|[in_terested, was...|            []|\n",
      "|           [5, vols]|                  []|        [vols]|\n",
      "|[on, the, night, ...|[stationed, crowd...|        [town]|\n",
      "|[by, hurried, and...|[carpenter, by, c...|[philadelphia]|\n",
      "+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the rows where doc contains the item '5'\n",
    "df_docs.where(F.array_contains('doc', '5')).show()\n",
    "\n",
    "# UDF removes items in TRIVIAL_TOKENS from array\n",
    "rm_trivial_udf = F.udf(lambda x:\n",
    "                     list(set(x) - TRIVIAL_TOKENS) if x\n",
    "                     else x,\n",
    "                     T.ArrayType(T.StringType()))\n",
    "\n",
    "# Remove trivial tokens from 'in' and 'out' columns of df2\n",
    "df_after = df_docs.withColumn('in_', rm_trivial_udf('in_'))\\\n",
    "                    .withColumn('out', rm_trivial_udf('out'))\n",
    "\n",
    "# Show the rows of df_after where doc contains the item '5'\n",
    "df_after.where(F.array_contains('doc','5')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vector data udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector, VectorUDT\n",
    "\n",
    "schema = T.StructType([T.StructField('output', VectorUDT())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector = spark\\\n",
    ".createDataFrame(\\\n",
    "[Row(output=SparseVector(12847, {65: 1.0})),\n",
    " Row(output=SparseVector(12847, {8: 1.0})),\n",
    " Row(output=SparseVector(12847, {47: 1.0})),\n",
    " Row(output=SparseVector(12847, {89: 1.0})),\n",
    " Row(output=SparseVector(12847, {94: 1.0})),\n",
    " Row(output=SparseVector(12847, {88: 1.0})),\n",
    " Row(output=SparseVector(12847, {43: 1.0})),\n",
    " Row(output=SparseVector(12847, {164: 1.0})),\n",
    " Row(output=SparseVector(12847, {91: 1.0})),\n",
    " Row(output=SparseVector(12847, {112: 1.0})),\n",
    " Row(output=SparseVector(12847, {97: 1.0})),\n",
    " Row(output=SparseVector(12847, {98: 1.0})),\n",
    " Row(output=SparseVector(12847, {48: 1.0})),\n",
    " Row(output=SparseVector(12847, {66: 1.0})),\n",
    " Row(output=SparseVector(12847, {74: 1.0})),\n",
    " Row(output=SparseVector(12847, {182: 1.0})),\n",
    " Row(output=SparseVector(12847, {118: 1.0})),\n",
    " Row(output=SparseVector(12847, {67: 1.0})),\n",
    " Row(output=SparseVector(12847, {53: 1.0})),\n",
    " Row(output=SparseVector(12847, {85: 1.0})),\n",
    " Row(output=SparseVector(12847, {2: 1.0})),\n",
    " Row(output=SparseVector(12847, {82: 1.0})),\n",
    " Row(output=SparseVector(12847, {29: 1.0})),\n",
    " Row(output=SparseVector(12847, {81: 1.0})),\n",
    " Row(output=SparseVector(12847, {80: 1.0})),\n",
    " Row(output=SparseVector(12847, {145: 1.0})),\n",
    " Row(output=SparseVector(12847, {117: 1.0})),\n",
    " Row(output=SparseVector(12847, {99: 1.0})),\n",
    " Row(output=SparseVector(12847, {72: 1.0})),\n",
    " Row(output=SparseVector(12847, {108: 1.0})),\n",
    " Row(output=SparseVector(12847, {147: 1.0})),\n",
    " Row(output=SparseVector(12847, {139: 1.0})),\n",
    " Row(output=SparseVector(12847, {191: 1.0})),\n",
    " Row(output=SparseVector(12847, {153: 1.0})),\n",
    " Row(output=SparseVector(12847, {175: 1.0})),\n",
    " Row(output=SparseVector(12847, {114: 1.0})),\n",
    " Row(output=SparseVector(12847, {56: 1.0})),\n",
    " Row(output=SparseVector(12847, {86: 1.0})),\n",
    " Row(output=SparseVector(12847, {38: 1.0})),\n",
    " Row(output=SparseVector(12847, {119: 1.0})),\n",
    " Row(output=SparseVector(12847, {95: 1.0})),\n",
    " Row(output=SparseVector(12847, {187: 1.0})),\n",
    " Row(output=SparseVector(12847, {25: 1.0})),\n",
    " Row(output=SparseVector(12847, {11: 1.0})),\n",
    " Row(output=SparseVector(12847, {168: 1.0})),\n",
    " Row(output=SparseVector(12847, {111: 1.0})),\n",
    " Row(output=SparseVector(12847, {123: 1.0})),\n",
    " Row(output=SparseVector(12847, {60: 1.0})),\n",
    " Row(output=SparseVector(12847, {57: 1.0})),\n",
    " Row(output=SparseVector(12847, {36: 1.0})),\n",
    " Row(output=SparseVector(12847, {63: 1.0})),\n",
    " Row(output=SparseVector(12847, {16: 1.0})),\n",
    " Row(output=SparseVector(12847, {110: 1.0})),\n",
    " Row(output=SparseVector(12847, {101: 1.0})),\n",
    " Row(output=SparseVector(12847, {5: 1.0})),\n",
    " Row(output=SparseVector(12847, {78: 1.0}))], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|result|\n",
      "+------+\n",
      "|  65.0|\n",
      "|   8.0|\n",
      "|  47.0|\n",
      "|  89.0|\n",
      "|  94.0|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selects the first element of a vector column\n",
    "first_udf = F.udf(lambda x:\n",
    "            float(x.indices[0]) \n",
    "            if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "            else 0.0,\n",
    "            T.FloatType())\n",
    "\n",
    "# Use select on df to apply first_udf to the output column\n",
    "df_vector.select(first_udf(\"output\").alias(\"result\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|            output|label|\n",
      "+------------------+-----+\n",
      "|(12847,[65],[1.0])| 65.0|\n",
      "| (12847,[8],[1.0])|  8.0|\n",
      "|(12847,[47],[1.0])| 47.0|\n",
      "|(12847,[89],[1.0])| 89.0|\n",
      "|(12847,[94],[1.0])| 94.0|\n",
      "+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add label by applying the first_udf to output column\n",
    "df_new = df_vector.withColumn('label', first_udf('output'))\n",
    "\n",
    "# Show the first five rows \n",
    "df_new.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform text to vector format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"words_in\", outputCol=\"vec\")\n",
    "df_docs = df_docs.withColumnRenamed(\"in_\", \"words_in\")\n",
    "model = cv.fit(df_docs)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"words_out\", outputCol=\"vec\")\n",
    "df_docs = df_docs.withColumnRenamed(\"out\", \"words_out\")\n",
    "model2 = cv.fit(df_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|            words_in|words_out|               invec|\n",
      "+--------------------+---------+--------------------+\n",
      "|[the, project, gu...|  [doyle]|(127,[0,4,6,7,8,1...|\n",
      "|[15, in_, our, se...|  [doyle]|(127,[2,7,14,24,2...|\n",
      "|[copyright, laws,...|  [world]|(127,[0,15,28,33,...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+---------------+\n",
      "|               invec|         outvec|\n",
      "+--------------------+---------------+\n",
      "|(127,[0,4,6,7,8,1...| (12,[2],[1.0])|\n",
      "|(127,[2,7,14,24,2...| (12,[2],[1.0])|\n",
      "|(127,[0,15,28,33,...|(12,[11],[1.0])|\n",
      "+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform df using model\n",
    "result = model.transform(df_docs.withColumnRenamed('in_', 'words'))\\\n",
    "        .withColumnRenamed('words', 'in')\\\n",
    "        .withColumnRenamed('vec', 'invec')\n",
    "result.drop('doc').show(3, True)\n",
    "\n",
    "# Add a column based on the out column called outvec\n",
    "result = model2.transform(result.withColumnRenamed('out', 'words'))\\\n",
    "        .withColumnRenamed('words', 'out')\\\n",
    "        .withColumnRenamed('vec', 'outvec')\n",
    "result.select('invec', 'outvec').show(3,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([T.StructField('endword', T.StringType()),\\\n",
    "                       T.StructField('doc', T.ArrayType(T.StringType())),\\\n",
    "                       T.StructField('features', VectorUDT()),\\\n",
    "                       T.StructField('outvec', VectorUDT())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = spark\\\n",
    ".createDataFrame(\\\n",
    "[('it', ['please', 'do', 'not', 'remove', 'it'], SparseVector(12847, {15: 1.0, 47: 1.0, 502: 1.0, 1515: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('holmes', ['start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'adventures', 'of', 'sherlock', 'holmes'], SparseVector(12847, {0: 1.0, 3: 1.0, 183: 1.0, 191: 1.0, 569: 1.0, 1584: 1.0, 1921: 1.0, 3302: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('i', ['the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'sir', 'arthur', 'conan', 'doyle', 'contents', 'i'], SparseVector(12847, {0: 1.0, 3: 1.0, 35: 1.0, 145: 1.0, 569: 1.0, 776: 1.0, 3270: 1.0, 3302: 1.0, 3647: 1.0, 8569: 1.0, 12351: 1.0}), SparseVector(12847, {11: 1.0})),\n",
    " ('i', ['the', 'adventure', 'of', 'the', 'copper', 'beeches', 'adventure', 'i'], SparseVector(12847, {0: 1.0, 3: 1.0, 3766: 1.0, 3830: 1.0, 6900: 1.0}), SparseVector(12847, {11: 1.0})),\n",
    " ('i', ['a', 'scandal', 'in', 'bohemia', 'i'], SparseVector(12847, {4: 1.0, 5: 1.0, 3669: 1.0, 5237: 1.0}), SparseVector(12847, {11: 1.0})),\n",
    " ('well', ['as', 'i', 'passed', 'the', 'well'], SparseVector(12847, {0: 1.0, 11: 1.0, 24: 1.0, 277: 1.0}), SparseVector(12847, {61: 1.0})),\n",
    " ('him', ['he', 'was', 'pacing', 'the', 'room', 'swiftly', 'eagerly', 'with', 'his', 'head', 'sunk', 'upon', 'his', 'chest', 'and', 'his', 'hands', 'clasped', 'behind', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 6: 1.0, 10: 1.0, 12: 1.0, 14: 1.0, 62: 1.0, 108: 1.0, 131: 1.0, 227: 1.0, 303: 1.0, 1737: 1.0, 1883: 1.0, 1941: 1.0, 1995: 1.0, 4485: 1.0, 5201: 1.0}), SparseVector(12847, {9: 1.0})),\n",
    " ('again', ['he', 'was', 'at', 'work', 'again'], SparseVector(12847, {6: 1.0, 10: 1.0, 18: 1.0, 369: 1.0}), SparseVector(12847, {91: 1.0})),\n",
    " ('me', ['but', 'he', 'was', 'glad', 'i', 'think', 'to', 'see', 'me'], SparseVector(12847, {2: 1.0, 6: 1.0, 10: 1.0, 11: 1.0, 19: 1.0, 87: 1.0, 159: 1.0, 420: 1.0}), SparseVector(12847, {34: 1.0})),\n",
    " ('you', ['i', 'think', 'watson', 'that', 'you', 'have', 'put', 'on', 'seven', 'and', 'a', 'half', 'pounds', 'since', 'i', 'saw', 'you'], SparseVector(12847, {1: 1.0, 4: 1.0, 8: 1.0, 11: 1.0, 13: 1.0, 20: 1.0, 30: 1.0, 159: 1.0, 192: 1.0, 231: 1.0, 353: 1.0, 506: 1.0, 767: 1.0, 1081: 1.0, 4490: 1.0}), SparseVector(12847, {13: 1.0})),\n",
    " ('know', ['then', 'how', 'do', 'you', 'know'], SparseVector(12847, {13: 1.0, 47: 1.0, 60: 1.0, 76: 1.0}), SparseVector(12847, {65: 1.0})),\n",
    " ('it', ['i', 'see', 'it', 'i', 'deduce', 'it'], SparseVector(12847, {7: 1.0, 11: 1.0, 87: 1.0, 2320: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('it', ['it', 'is', 'true', 'that', 'i', 'had', 'a', 'country', 'walk', 'on', 'thursday', 'and', 'came', 'home', 'in', 'a', 'dreadful', 'mess', 'but', 'as', 'i', 'have', 'changed', 'my', 'clothes', 'i', 'canot', 'imagine', 'how', 'you', 'deduce', 'it'], SparseVector(12847, {1: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 8: 1.0, 11: 1.0, 13: 1.0, 16: 1.0, 17: 1.0, 19: 1.0, 20: 1.0, 24: 1.0, 30: 1.0, 64: 1.0, 76: 1.0, 105: 1.0, 164: 1.0, 333: 1.0, 402: 1.0, 543: 1.0, 650: 1.0, 829: 1.0, 983: 1.0, 1155: 1.0, 1205: 1.0, 2320: 1.0, 5670: 1.0, 6863: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('out', ['as', 'to', 'mary', 'jane', 'she', 'is', 'incorrigible', 'and', 'my', 'wife', 'has', 'given', 'her', 'notice', 'but', 'there', 'again', 'i', 'fail', 'to', 'see', 'how', 'you', 'work', 'it', 'out'], SparseVector(12847, {1: 1.0, 2: 1.0, 7: 1.0, 11: 1.0, 13: 1.0, 16: 1.0, 19: 1.0, 21: 1.0, 24: 1.0, 26: 1.0, 38: 1.0, 64: 1.0, 76: 1.0, 83: 1.0, 87: 1.0, 91: 1.0, 143: 1.0, 266: 1.0, 355: 1.0, 369: 1.0, 719: 1.0, 2591: 1.0, 7625: 1.0, 8032: 1.0}), SparseVector(12847, {48: 1.0})),\n",
    " ('he', ['it', 'is', 'simplicity', 'itself', 'said', 'he'], SparseVector(12847, {7: 1.0, 16: 1.0, 23: 1.0, 436: 1.0, 3642: 1.0}), SparseVector(12847, {6: 1.0})),\n",
    " ('it', ['obviously', 'they', 'have', 'been', 'caused', 'by', 'someone', 'who', 'has', 'very', 'carelessly', 'scraped', 'round', 'the', 'edges', 'of', 'the', 'sole', 'in', 'order', 'to', 'remove', 'crusted', 'mud', 'from', 'it'], SparseVector(12847, {0: 1.0, 2: 1.0, 3: 1.0, 5: 1.0, 28: 1.0, 30: 1.0, 35: 1.0, 40: 1.0, 41: 1.0, 55: 1.0, 83: 1.0, 84: 1.0, 208: 1.0, 324: 1.0, 520: 1.0, 999: 1.0, 1515: 1.0, 1671: 1.0, 1777: 1.0, 2349: 1.0, 3048: 1.0, 3668: 1.0, 8444: 1.0, 12689: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('room', ['for', 'example', 'you', 'have', 'frequently', 'seen', 'the', 'steps', 'which', 'lead', 'up', 'from', 'the', 'hall', 'to', 'this', 'room'], SparseVector(12847, {0: 1.0, 2: 1.0, 13: 1.0, 22: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 39: 1.0, 43: 1.0, 249: 1.0, 561: 1.0, 699: 1.0, 740: 1.0, 1116: 1.0, 1565: 1.0}), SparseVector(12847, {62: 1.0})),\n",
    " ('there', ['then', 'how', 'many', 'are', 'there'], SparseVector(12847, {52: 1.0, 60: 1.0, 76: 1.0, 230: 1.0}), SparseVector(12847, {38: 1.0})),\n",
    " ('know', ['i', 'donot', 'know'], SparseVector(12847, {11: 1.0, 133: 1.0}), SparseVector(12847, {65: 1.0})),\n",
    " ('so', ['quite', 'so'], SparseVector(12847, {203: 1.0}), SparseVector(12847, {32: 1.0})),\n",
    " ('this', ['by', 'the', 'way', 'since', 'you', 'are', 'interested', 'in', 'these', 'little', 'problems', 'and', 'since', 'you', 'are', 'good', 'enough', 'to', 'chronicle', 'one', 'or', 'two', 'of', 'my', 'trifling', 'experiences', 'you', 'may', 'be', 'interested', 'in', 'this'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 5: 1.0, 13: 1.0, 27: 1.0, 35: 1.0, 36: 1.0, 44: 1.0, 52: 1.0, 64: 1.0, 86: 1.0, 99: 1.0, 106: 1.0, 130: 1.0, 137: 1.0, 156: 1.0, 353: 1.0, 687: 1.0, 1191: 1.0, 1837: 1.0, 3556: 1.0, 4435: 1.0, 10616: 1.0}), SparseVector(12847, {29: 1.0})),\n",
    " ('he', ['it', 'came', 'by', 'the', 'last', 'post', 'said', 'he'], SparseVector(12847, {0: 1.0, 7: 1.0, 23: 1.0, 35: 1.0, 105: 1.0, 225: 1.0, 1035: 1.0}), SparseVector(12847, {6: 1.0})),\n",
    " ('to', ['there', 'will', 'call', 'upon', 'you', 'to'], SparseVector(12847, {13: 1.0, 38: 1.0, 68: 1.0, 131: 1.0, 514: 1.0}), SparseVector(12847, {2: 1.0})),\n",
    " ('it', ['what', 'do', 'you', 'deduce', 'from', 'it'], SparseVector(12847, {13: 1.0, 28: 1.0, 31: 1.0, 47: 1.0, 2320: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('holmes', ['that', 'is', 'the', 'very', 'word', 'said', 'holmes'], SparseVector(12847, {0: 1.0, 8: 1.0, 16: 1.0, 23: 1.0, 84: 1.0, 340: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('all', ['it', 'is', 'not', 'an', 'english', 'paper', 'at', 'all'], SparseVector(12847, {7: 1.0, 15: 1.0, 16: 1.0, 18: 1.0, 51: 1.0, 796: 1.0, 918: 1.0}), SparseVector(12847, {25: 1.0})),\n",
    " ('that', ['what', 'do', 'you', 'make', 'of', 'that'], SparseVector(12847, {3: 1.0, 13: 1.0, 31: 1.0, 47: 1.0, 218: 1.0}), SparseVector(12847, {8: 1.0})),\n",
    " ('holmes', ['asked', 'holmes'], SparseVector(12847, {80: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('all', ['not', 'at', 'all'], SparseVector(12847, {15: 1.0, 18: 1.0}), SparseVector(12847, {25: 1.0})),\n",
    " ('country', ['speaking', 'country'], SparseVector(12847, {367: 1.0}), SparseVector(12847, {164: 1.0})),\n",
    " ('that', ['ha', 'ha', 'my', 'boy', 'what', 'do', 'you', 'make', 'of', 'that'], SparseVector(12847, {3: 1.0, 13: 1.0, 31: 1.0, 47: 1.0, 64: 1.0, 218: 1.0, 578: 1.0, 6157: 1.0}), SparseVector(12847, {8: 1.0})),\n",
    " ('said', ['the', 'paper', 'was', 'made', 'in', 'bohemia', 'i', 'said'], SparseVector(12847, {0: 1.0, 5: 1.0, 10: 1.0, 11: 1.0, 128: 1.0, 796: 1.0, 5237: 1.0}), SparseVector(12847, {23: 1.0})),\n",
    " ('that', ['a', 'frenchman', 'or', 'russian', 'could', 'not', 'have', 'written', 'that'], SparseVector(12847, {4: 1.0, 15: 1.0, 30: 1.0, 44: 1.0, 73: 1.0, 239: 1.0, 576: 1.0, 916: 1.0}), SparseVector(12847, {8: 1.0})),\n",
    " ('face', ['it', 'only', 'remains', 'therefore', 'to', 'discover', 'what', 'is', 'wanted', 'by', 'this', 'german', 'who', 'writes', 'upon', 'bohemian', 'paper', 'and', 'prefers', 'wearing', 'a', 'mask', 'to', 'showing', 'his', 'face'], SparseVector(12847, {1: 1.0, 2: 1.0, 4: 1.0, 7: 1.0, 14: 1.0, 16: 1.0, 29: 1.0, 31: 1.0, 35: 1.0, 41: 1.0, 71: 1.0, 131: 1.0, 313: 1.0, 796: 1.0, 956: 1.0, 997: 1.0, 1074: 1.0, 1576: 1.0, 1719: 1.0, 2806: 1.0, 4001: 1.0, 5932: 1.0, 12256: 1.0, 12258: 1.0}), SparseVector(12847, {74: 1.0})),\n",
    " ('he', ['a', 'pair', 'by', 'the', 'sound', 'said', 'he'], SparseVector(12847, {0: 1.0, 4: 1.0, 23: 1.0, 35: 1.0, 449: 1.0, 2499: 1.0}), SparseVector(12847, {6: 1.0})),\n",
    " ('holmes', ['i', 'think', 'that', 'i', 'had', 'better', 'go', 'holmes'], SparseVector(12847, {8: 1.0, 11: 1.0, 17: 1.0, 104: 1.0, 159: 1.0, 323: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('it', ['it', 'would', 'be', 'a', 'pity', 'to', 'miss', 'it'], SparseVector(12847, {2: 1.0, 4: 1.0, 7: 1.0, 27: 1.0, 59: 1.0, 822: 1.0, 1287: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('him', ['never', 'mind', 'him'], SparseVector(12847, {157: 1.0, 325: 1.0}), SparseVector(12847, {9: 1.0})),\n",
    " ('he', ['i', 'may', 'want', 'your', 'help', 'and', 'so', 'may', 'he'], SparseVector(12847, {1: 1.0, 11: 1.0, 32: 1.0, 106: 1.0, 124: 1.0, 246: 1.0, 291: 1.0}), SparseVector(12847, {6: 1.0})),\n",
    " ('door', ['a', 'slow', 'and', 'heavy', 'step', 'which', 'had', 'been', 'heard', 'upon', 'the', 'stairs', 'and', 'in', 'the', 'passage', 'paused', 'immediately', 'outside', 'the', 'door'], SparseVector(12847, {0: 1.0, 1: 1.0, 4: 1.0, 5: 1.0, 17: 1.0, 39: 1.0, 55: 1.0, 131: 1.0, 193: 1.0, 549: 1.0, 664: 1.0, 973: 1.0, 1001: 1.0, 1098: 1.0, 1854: 1.0, 2659: 1.0, 4817: 1.0}), SparseVector(12847, {118: 1.0})),\n",
    " ('in', ['come', 'in'], SparseVector(12847, {93: 1.0}), SparseVector(12847, {5: 1.0})),\n",
    " ('holmes', ['said', 'holmes'], SparseVector(12847, {23: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('holmes', ['pray', 'take', 'a', 'seat', 'said', 'holmes'], SparseVector(12847, {4: 1.0, 23: 1.0, 174: 1.0, 712: 1.0, 972: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('he', ['it', 'is', 'both', 'or', 'none', 'said', 'he'], SparseVector(12847, {7: 1.0, 16: 1.0, 23: 1.0, 44: 1.0, 268: 1.0, 749: 1.0}), SparseVector(12847, {6: 1.0})),\n",
    " ('me', ['you', 'may', 'say', 'before', 'this', 'gentleman', 'anything', 'which', 'you', 'may', 'say', 'to', 'me'], SparseVector(12847, {2: 1.0, 13: 1.0, 29: 1.0, 39: 1.0, 67: 1.0, 81: 1.0, 106: 1.0, 237: 1.0, 1149: 1.0}), SparseVector(12847, {34: 1.0})),\n",
    " ('holmes', ['i', 'promise', 'said', 'holmes'], SparseVector(12847, {11: 1.0, 23: 1.0, 1153: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('i', ['and', 'i'], SparseVector(12847, {1: 1.0}), SparseVector(12847, {11: 1.0})),\n",
    " ('good', ['then', 'i', 'rather', 'imprudently', 'wished', 'you', 'good'], SparseVector(12847, {11: 1.0, 13: 1.0, 60: 1.0, 302: 1.0, 528: 1.0, 9310: 1.0}), SparseVector(12847, {99: 1.0})),\n",
    " ('to', ['so', 'you', 'will', 'find', 'the', 'nest', 'empty', 'when', 'you', 'call', 'to'], SparseVector(12847, {0: 1.0, 13: 1.0, 32: 1.0, 45: 1.0, 68: 1.0, 301: 1.0, 514: 1.0, 2058: 1.0, 11076: 1.0}), SparseVector(12847, {2: 1.0})),\n",
    " ('he', ['i', 'love', 'and', 'am', 'loved', 'by', 'a', 'better', 'man', 'than', 'he'], SparseVector(12847, {1: 1.0, 4: 1.0, 11: 1.0, 35: 1.0, 53: 1.0, 113: 1.0, 120: 1.0, 166: 1.0, 323: 1.0, 423: 1.0}), SparseVector(12847, {6: 1.0})),\n",
    " ('so', ['i', 'am', 'glad', 'to', 'hear', 'your', 'majesty', 'say', 'so'], SparseVector(12847, {2: 1.0, 11: 1.0, 81: 1.0, 120: 1.0, 124: 1.0, 396: 1.0, 420: 1.0, 1124: 1.0}), SparseVector(12847, {32: 1.0})),\n",
    " ('you', ['i', 'am', 'immensely', 'indebted', 'to', 'you'], SparseVector(12847, {2: 1.0, 11: 1.0, 120: 1.0, 4039: 1.0, 6011: 1.0}), SparseVector(12847, {13: 1.0})),\n",
    " ('you', ['pray', 'tell', 'me', 'in', 'what', 'way', 'i', 'can', 'reward', 'you'], SparseVector(12847, {5: 1.0, 11: 1.0, 31: 1.0, 34: 1.0, 86: 1.0, 129: 1.0, 160: 1.0, 712: 1.0, 2122: 1.0}), SparseVector(12847, {13: 1.0})),\n",
    " ('hand', ['he', 'slipped', 'an', 'emerald', 'snake', 'ring', 'from', 'his', 'finger', 'and', 'held', 'it', 'out', 'upon', 'the', 'palm', 'of', 'his', 'hand'], SparseVector(12847, {0: 1.0, 1: 1.0, 3: 1.0, 6: 1.0, 7: 1.0, 14: 1.0, 28: 1.0, 48: 1.0, 51: 1.0, 131: 1.0, 296: 1.0, 1309: 1.0, 1665: 1.0, 1762: 1.0, 2336: 1.0, 5166: 1.0, 7782: 1.0}), SparseVector(12847, {82: 1.0})),\n",
    " ('holmes', ['your', 'majesty', 'has', 'something', 'which', 'i', 'should', 'value', 'even', 'more', 'highly', 'said', 'holmes'], SparseVector(12847, {11: 1.0, 23: 1.0, 39: 1.0, 83: 1.0, 92: 1.0, 109: 1.0, 124: 1.0, 142: 1.0, 161: 1.0, 1079: 1.0, 1124: 1.0, 1845: 1.0}), SparseVector(12847, {145: 1.0})),\n",
    " ('it', ['you', 'have', 'but', 'to', 'name', 'it'], SparseVector(12847, {2: 1.0, 13: 1.0, 19: 1.0, 30: 1.0, 484: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('it', ['certainly', 'if', 'you', 'wish', 'it'], SparseVector(12847, {13: 1.0, 49: 1.0, 257: 1.0, 633: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('me', ['with', 'an', 'apology', 'for', 'my', 'intrusion', 'i', 'was', 'about', 'to', 'withdraw', 'when', 'holmes', 'pulled', 'me', 'abruptly', 'into', 'the', 'room', 'and', 'closed', 'the', 'door', 'behind', 'me'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 22: 1.0, 34: 1.0, 45: 1.0, 51: 1.0, 62: 1.0, 64: 1.0, 69: 1.0, 70: 1.0, 118: 1.0, 145: 1.0, 227: 1.0, 415: 1.0, 1216: 1.0, 3875: 1.0, 4310: 1.0, 4549: 1.0, 10464: 1.0}), SparseVector(12847, {34: 1.0})),\n",
    " ('so', ['very', 'much', 'so'], SparseVector(12847, {84: 1.0, 171: 1.0}), SparseVector(12847, {32: 1.0})),\n",
    " ('room', ['then', 'i', 'can', 'wait', 'in', 'the', 'next', 'room'], SparseVector(12847, {0: 1.0, 5: 1.0, 11: 1.0, 60: 1.0, 129: 1.0, 293: 1.0, 660: 1.0}), SparseVector(12847, {62: 1.0})),\n",
    " ('all', ['not', 'at', 'all'], SparseVector(12847, {15: 1.0, 18: 1.0}), SparseVector(12847, {25: 1.0})),\n",
    " ('eyes', ['encircled', 'eyes'], SparseVector(12847, {10811: 1.0}), SparseVector(12847, {72: 1.0})),\n",
    " ('i', ['the', 'adventure', 'of', 'the', 'copper', 'beeches', 'adventure', 'i'], SparseVector(12847, {0: 1.0, 3: 1.0, 3766: 1.0, 3830: 1.0, 6900: 1.0}), SparseVector(12847, {11: 1.0})),\n",
    " ('he', ['it', 'is', 'simplicity', 'itself', 'said', 'he'], SparseVector(12847, {7: 1.0, 16: 1.0, 23: 1.0, 436: 1.0, 3642: 1.0}), SparseVector(12847, {6: 1.0})),\n",
    " ('it', ['why', 'should', 'i', 'attempt', 'to', 'conceal', 'it'], SparseVector(12847, {2: 1.0, 11: 1.0, 109: 1.0, 140: 1.0, 1469: 1.0, 1738: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('it', ['and', 'she', 'will', 'do', 'it'], SparseVector(12847, {1: 1.0, 26: 1.0, 47: 1.0, 68: 1.0}), SparseVector(12847, {7: 1.0})),\n",
    " ('said', ['there', 'are', 'three', 'hundred', 'pounds', 'in', 'gold', 'and', 'seven', 'hundred', 'in', 'notes', 'he', 'said'], SparseVector(12847, {1: 1.0, 5: 1.0, 6: 1.0, 38: 1.0, 52: 1.0, 314: 1.0, 560: 1.0, 767: 1.0, 1369: 1.0, 2476: 1.0, 4490: 1.0}), SparseVector(12847, {23: 1.0})),\n",
    " ('life', ['i', 'know', 'my', 'dear', 'watson', 'that', 'you', 'share', 'my', 'love', 'of', 'all', 'that', 'is', 'bizarre', 'and', 'outside', 'the', 'conventions', 'and', 'humdrum', 'routine', 'of', 'everyday', 'life'], SparseVector(12847, {0: 1.0, 1: 1.0, 3: 1.0, 8: 1.0, 11: 1.0, 13: 1.0, 16: 1.0, 25: 1.0, 64: 1.0, 65: 1.0, 166: 1.0, 263: 1.0, 664: 1.0, 1081: 1.0, 1245: 1.0, 3981: 1.0, 4124: 1.0, 4266: 1.0, 8342: 1.0, 11111: 1.0}), SparseVector(12847, {79: 1.0})),\n",
    " ('right', ['you', 'did', 'doctor', 'but', 'none', 'the', 'less', 'you', 'must', 'come', 'round', 'to', 'my', 'view', 'for', 'otherwise', 'i', 'shall', 'keep', 'on', 'piling', 'fact', 'upon', 'fact', 'on', 'you', 'until', 'your', 'reason', 'breaks', 'down', 'under', 'them', 'and', 'acknowledges', 'me', 'to', 'be', 'right'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 11: 1.0, 13: 1.0, 19: 1.0, 20: 1.0, 22: 1.0, 27: 1.0, 33: 1.0, 34: 1.0, 50: 1.0, 64: 1.0, 85: 1.0, 93: 1.0, 124: 1.0, 131: 1.0, 152: 1.0, 176: 1.0, 184: 1.0, 208: 1.0, 393: 1.0, 451: 1.0, 463: 1.0, 467: 1.0, 579: 1.0, 651: 1.0, 749: 1.0, 757: 1.0, 2033: 1.0, 4396: 1.0, 7049: 1.0, 12739: 1.0}), SparseVector(12847, {117: 1.0}))],\\\n",
    " schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  4\n",
      "+-------+--------------------+--------------------+-------------------+-----+\n",
      "|endword|                 doc|            features|             outvec|label|\n",
      "+-------+--------------------+--------------------+-------------------+-----+\n",
      "|     it|[please, do, not,...|(12847,[15,47,502...|  (12847,[7],[1.0])|    0|\n",
      "| holmes|[start, of, the, ...|(12847,[0,3,183,1...|(12847,[145],[1.0])|    0|\n",
      "+-------+--------------------+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the rows where endword is 'him' and label 1\n",
    "df_pos = df_class.where(\"endword = 'him'\")\\\n",
    "           .withColumn('label', F.lit(1))\n",
    "\n",
    "# Select the rows where endword is not 'him' and label 0\n",
    "df_neg = df_class.where(\"endword <> 'him'\")\\\n",
    "           .withColumn('label', F.lit(0))\n",
    "\n",
    "# Union pos and neg in equal number\n",
    "df_examples = df_pos.union(df_neg.limit(df_pos.count()))\n",
    "print(\"Number of examples: \", df_examples.count())\n",
    "df_examples.where(\"endword <> 'him'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = T.StructType([T.StructField('endword', T.StringType()),\\\n",
    "                       T.StructField('doc', T.ArrayType(T.StringType())),\\\n",
    "                       T.StructField('features', VectorUDT()),\\\n",
    "                       T.StructField('outvec', VectorUDT()),\\\n",
    "                       T.StructField('label', T.IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = spark\\\n",
    ".createDataFrame(\\\n",
    "[('him', ['he', 'was', 'pacing', 'the', 'room', 'swiftly', 'eagerly', 'with', 'his', 'head', 'sunk', 'upon', 'his', 'chest', 'and', 'his', 'hands', 'clasped', 'behind', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 6: 1.0, 10: 1.0, 12: 1.0, 14: 1.0, 62: 1.0, 108: 1.0, 131: 1.0, 227: 1.0, 303: 1.0, 1737: 1.0, 1883: 1.0, 1941: 1.0, 1995: 1.0, 4485: 1.0, 5201: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['never', 'mind', 'him'], SparseVector(12847, {157: 1.0, 325: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['book', 'and', 'handed', 'it', 'to', 'him'], SparseVector(12847, {1: 1.0, 2: 1.0, 7: 1.0, 799: 1.0, 1285: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['has', 'only', 'one', 'male', 'visitor', 'but', 'a', 'good', 'deal', 'of', 'him'], SparseVector(12847, {3: 1.0, 4: 1.0, 19: 1.0, 36: 1.0, 71: 1.0, 83: 1.0, 99: 1.0, 1033: 1.0, 1657: 1.0, 1843: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['mews', 'and', 'knew', 'all', 'about', 'him'], SparseVector(12847, {1: 1.0, 25: 1.0, 69: 1.0, 190: 1.0, 5920: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['i', 'shall', 'drive', 'out', 'in', 'the', 'park', 'at', 'five', 'as', 'usual', 'she', 'said', 'as', 'she', 'left', 'him'], SparseVector(12847, {0: 1.0, 5: 1.0, 11: 1.0, 18: 1.0, 23: 1.0, 24: 1.0, 26: 1.0, 48: 1.0, 136: 1.0, 176: 1.0, 508: 1.0, 689: 1.0, 702: 1.0, 9693: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['no', 'no', 'there', 's', 'life', 'in', 'him'], SparseVector(12847, {5: 1.0, 38: 1.0, 54: 1.0, 79: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['they', 'would', 'have', 'had', 'the', 'lady', 's', 'purse', 'and', 'watch', 'if', 'it', 'hadnot', 'been', 'for', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 7: 1.0, 17: 1.0, 22: 1.0, 30: 1.0, 40: 1.0, 49: 1.0, 55: 1.0, 59: 1.0, 397: 1.0, 1972: 1.0, 1978: 1.0, 3013: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['but', 'she', 'could', 'not', 'love', 'him'], SparseVector(12847, {15: 1.0, 19: 1.0, 26: 1.0, 73: 1.0, 166: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['hat', 'and', 'a', 'faded', 'brown', 'overcoat', 'with', 'a', 'wrinkled', 'velvet', 'collar', 'lay', 'upon', 'a', 'chair', 'beside', 'him'], SparseVector(12847, {1: 1.0, 4: 1.0, 12: 1.0, 131: 1.0, 287: 1.0, 288: 1.0, 752: 1.0, 1006: 1.0, 2900: 1.0, 3951: 1.0, 4273: 1.0, 4781: 1.0, 5134: 1.0, 9128: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['and', 'i', 'know', 'very', 'well', 'that', 'he', 'could', 'better', 'himself', 'and', 'earn', 'twice', 'what', 'i', 'am', 'able', 'to', 'give', 'him'], SparseVector(12847, {1: 1.0, 2: 1.0, 6: 1.0, 8: 1.0, 11: 1.0, 31: 1.0, 61: 1.0, 63: 1.0, 65: 1.0, 73: 1.0, 84: 1.0, 120: 1.0, 242: 1.0, 323: 1.0, 448: 1.0, 1072: 1.0, 4891: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['there', 's', 'no', 'vice', 'in', 'him'], SparseVector(12847, {5: 1.0, 38: 1.0, 54: 1.0, 10753: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['he', 'answered', 'that', 'the', 'name', 'was', 'new', 'to', 'him'], SparseVector(12847, {0: 1.0, 2: 1.0, 6: 1.0, 8: 1.0, 10: 1.0, 188: 1.0, 479: 1.0, 484: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['where', 'could', 'i', 'find', 'him'], SparseVector(12847, {11: 1.0, 73: 1.0, 151: 1.0, 301: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['why', 'did', 'you', 'pick', 'him'], SparseVector(12847, {13: 1.0, 50: 1.0, 140: 1.0, 2520: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['i', 'have', 'only', 'just', 'left', 'him'], SparseVector(12847, {11: 1.0, 30: 1.0, 71: 1.0, 136: 1.0, 144: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['i', 'am', 'sure', 'that', 'you', 'inquired', 'your', 'way', 'merely', 'in', 'order', 'that', 'you', 'might', 'see', 'him'], SparseVector(12847, {5: 1.0, 8: 1.0, 11: 1.0, 13: 1.0, 86: 1.0, 87: 1.0, 120: 1.0, 124: 1.0, 195: 1.0, 324: 1.0, 428: 1.0, 468: 1.0, 1902: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['not', 'him'], SparseVector(12847, {15: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['in', 'his', 'singular', 'character', 'the', 'dual', 'nature', 'alternately', 'asserted', 'itself', 'and', 'his', 'extreme', 'exactness', 'and', 'astuteness', 'represented', 'as', 'i', 'have', 'often', 'thought', 'the', 'reaction', 'against', 'the', 'poetic', 'and', 'contemplative', 'mood', 'which', 'occasionally', 'predominated', 'in', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 5: 1.0, 11: 1.0, 14: 1.0, 24: 1.0, 30: 1.0, 39: 1.0, 102: 1.0, 213: 1.0, 354: 1.0, 436: 1.0, 886: 1.0, 1140: 1.0, 1193: 1.0, 1405: 1.0, 2166: 1.0, 2173: 1.0, 2314: 1.0, 2825: 1.0, 3493: 1.0, 5568: 1.0, 7470: 1.0, 9317: 1.0, 10383: 1.0, 10685: 1.0, 11812: 1.0, 12002: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['he', 'has', 'his', 'own', 'little', 'methods', 'which', 'are', 'if', 'he', 'wonot', 'mind', 'my', 'saying', 'so', 'just', 'a', 'little', 'too', 'theoretical', 'and', 'fantastic', 'but', 'he', 'has', 'the', 'makings', 'of', 'a', 'detective', 'in', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 14: 1.0, 19: 1.0, 32: 1.0, 39: 1.0, 49: 1.0, 52: 1.0, 64: 1.0, 83: 1.0, 137: 1.0, 144: 1.0, 167: 1.0, 248: 1.0, 325: 1.0, 330: 1.0, 398: 1.0, 2831: 1.0, 3741: 1.0, 10855: 1.0, 11837: 1.0, 12806: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['you', 'are', 'not', 'very', 'vulnerable', 'from', 'above', 'holmes', 'remarked', 'as', 'he', 'held', 'up', 'the', 'lantern', 'and', 'gazed', 'about', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 6: 1.0, 13: 1.0, 15: 1.0, 24: 1.0, 28: 1.0, 43: 1.0, 52: 1.0, 69: 1.0, 84: 1.0, 145: 1.0, 296: 1.0, 381: 1.0, 581: 1.0, 743: 1.0, 7397: 1.0, 10657: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['sherlock', 'holmes', 'welcomed', 'her', 'with', 'the', 'easy', 'courtesy', 'for', 'which', 'he', 'was', 'remarkable', 'and', 'having', 'closed', 'the', 'door', 'and', 'bowed', 'her', 'into', 'an', 'armchair', 'he', 'looked', 'her', 'over', 'in', 'the', 'minute', 'and', 'yet', 'abstracted', 'fashion', 'which', 'was', 'peculiar', 'to', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 5: 1.0, 6: 1.0, 10: 1.0, 12: 1.0, 21: 1.0, 22: 1.0, 39: 1.0, 51: 1.0, 70: 1.0, 116: 1.0, 118: 1.0, 121: 1.0, 145: 1.0, 181: 1.0, 251: 1.0, 415: 1.0, 569: 1.0, 913: 1.0, 958: 1.0, 1041: 1.0, 1236: 1.0, 1581: 1.0, 1881: 1.0, 3396: 1.0, 4878: 1.0, 6148: 1.0, 6623: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['i', 'met', 'him', 'that', 'night', 'and', 'he', 'called', 'next', 'day', 'to', 'ask', 'if', 'we', 'had', 'got', 'home', 'all', 'safe', 'and', 'after', 'that', 'we', 'met', 'him'], SparseVector(12847, {1: 1.0, 2: 1.0, 6: 1.0, 8: 1.0, 9: 1.0, 11: 1.0, 17: 1.0, 25: 1.0, 49: 1.0, 75: 1.0, 100: 1.0, 111: 1.0, 267: 1.0, 292: 1.0, 293: 1.0, 298: 1.0, 315: 1.0, 339: 1.0, 402: 1.0, 2390: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['he', 'was', 'in', 'dreadful', 'earnest', 'and', 'made', 'me', 'swear', 'with', 'my', 'hands', 'on', 'the', 'testament', 'that', 'whatever', 'happened', 'i', 'would', 'always', 'be', 'true', 'to', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 5: 1.0, 6: 1.0, 8: 1.0, 10: 1.0, 11: 1.0, 12: 1.0, 20: 1.0, 27: 1.0, 34: 1.0, 59: 1.0, 64: 1.0, 128: 1.0, 194: 1.0, 303: 1.0, 404: 1.0, 543: 1.0, 930: 1.0, 1155: 1.0, 4265: 1.0, 8801: 1.0, 11234: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['but', 'they', 'both', 'said', 'never', 'to', 'mind', 'about', 'father', 'but', 'just', 'to', 'tell', 'him', 'afterwards', 'and', 'mother', 'said', 'she', 'would', 'make', 'it', 'all', 'right', 'with', 'him'], SparseVector(12847, {1: 1.0, 2: 1.0, 7: 1.0, 9: 1.0, 12: 1.0, 19: 1.0, 23: 1.0, 25: 1.0, 26: 1.0, 40: 1.0, 59: 1.0, 69: 1.0, 117: 1.0, 144: 1.0, 157: 1.0, 160: 1.0, 212: 1.0, 218: 1.0, 268: 1.0, 280: 1.0, 325: 1.0, 1385: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['that', 'was', 'last', 'friday', 'mr', 'holmes', 'and', 'i', 'have', 'never', 'seen', 'or', 'heard', 'anything', 'since', 'then', 'to', 'throw', 'any', 'light', 'upon', 'what', 'became', 'of', 'him'], SparseVector(12847, {1: 1.0, 2: 1.0, 3: 1.0, 8: 1.0, 10: 1.0, 11: 1.0, 30: 1.0, 31: 1.0, 44: 1.0, 60: 1.0, 125: 1.0, 131: 1.0, 145: 1.0, 157: 1.0, 193: 1.0, 225: 1.0, 237: 1.0, 249: 1.0, 252: 1.0, 353: 1.0, 414: 1.0, 431: 1.0, 2553: 1.0, 4284: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['your', 'own', 'opinion', 'is', 'then', 'that', 'some', 'unforeseen', 'catastrophe', 'has', 'occurred', 'to', 'him'], SparseVector(12847, {2: 1.0, 8: 1.0, 16: 1.0, 60: 1.0, 83: 1.0, 96: 1.0, 124: 1.0, 167: 1.0, 838: 1.0, 890: 1.0, 8739: 1.0, 10507: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['did', 'you', 'tell', 'him'], SparseVector(12847, {13: 1.0, 50: 1.0, 160: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['then', 'what', 'has', 'happened', 'to', 'him'], SparseVector(12847, {2: 1.0, 31: 1.0, 60: 1.0, 83: 1.0, 404: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['here', 'is', 'the', 'slip', 'and', 'here', 'are', 'four', 'letters', 'from', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 16: 1.0, 28: 1.0, 52: 1.0, 97: 1.0, 442: 1.0, 810: 1.0, 4094: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['tubes', 'with', 'the', 'pungent', 'cleanly', 'smell', 'of', 'hydrochloric', 'acid', 'told', 'me', 'that', 'he', 'had', 'spent', 'his', 'day', 'in', 'the', 'chemical', 'work', 'which', 'was', 'so', 'dear', 'to', 'him'], SparseVector(12847, {0: 1.0, 2: 1.0, 3: 1.0, 5: 1.0, 6: 1.0, 8: 1.0, 10: 1.0, 12: 1.0, 14: 1.0, 17: 1.0, 32: 1.0, 34: 1.0, 39: 1.0, 111: 1.0, 186: 1.0, 263: 1.0, 369: 1.0, 706: 1.0, 2471: 1.0, 4512: 1.0, 5532: 1.0, 5916: 1.0, 6501: 1.0, 9365: 1.0, 12459: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['i', 'let', 'you', 'know', 'then', 'that', 'i', 'have', 'caught', 'him'], SparseVector(12847, {8: 1.0, 11: 1.0, 13: 1.0, 30: 1.0, 60: 1.0, 65: 1.0, 173: 1.0, 949: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['we', 'had', 'the', 'carriage', 'to', 'ourselves', 'save', 'for', 'an', 'immense', 'litter', 'of', 'papers', 'which', 'holmes', 'had', 'brought', 'with', 'him'], SparseVector(12847, {0: 1.0, 2: 1.0, 3: 1.0, 12: 1.0, 17: 1.0, 22: 1.0, 39: 1.0, 51: 1.0, 75: 1.0, 145: 1.0, 250: 1.0, 604: 1.0, 1040: 1.0, 1168: 1.0, 1378: 1.0, 1402: 1.0, 8533: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['to', 'the', 'best', 'of', 'his', 'belief', 'the', 'father', 'was', 'actually', 'in', 'sight', 'at', 'the', 'time', 'and', 'the', 'son', 'was', 'following', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 5: 1.0, 10: 1.0, 14: 1.0, 18: 1.0, 58: 1.0, 212: 1.0, 328: 1.0, 446: 1.0, 517: 1.0, 648: 1.0, 1516: 1.0, 4473: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['however', 'innocent', 'he', 'might', 'be', 'he', 'could', 'not', 'be', 'such', 'an', 'absolute', 'imbecile', 'as', 'not', 'to', 'see', 'that', 'the', 'circumstances', 'were', 'very', 'black', 'against', 'him'], SparseVector(12847, {0: 1.0, 2: 1.0, 6: 1.0, 8: 1.0, 15: 1.0, 24: 1.0, 27: 1.0, 42: 1.0, 51: 1.0, 73: 1.0, 84: 1.0, 87: 1.0, 134: 1.0, 195: 1.0, 213: 1.0, 295: 1.0, 457: 1.0, 1262: 1.0, 1961: 1.0, 2255: 1.0, 7683: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['as', 'to', 'his', 'remark', 'about', 'his', 'deserts', 'it', 'was', 'also', 'not', 'unnatural', 'if', 'you', 'consider', 'that', 'he', 'stood', 'beside', 'the', 'dead', 'body', 'of', 'his', 'father', 'and', 'that', 'there', 'is', 'no', 'doubt', 'that', 'he', 'had', 'that', 'very', 'day', 'so', 'far', 'forgotten', 'his', 'filial', 'duty', 'as', 'to', 'bandy', 'words', 'with', 'him', 'and', 'even', 'according', 'to', 'the', 'little', 'girl', 'whose', 'evidence', 'is', 'so', 'important', 'to', 'raise', 'his', 'hand', 'as', 'if', 'to', 'strike', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0, 10: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, 15: 1.0, 16: 1.0, 17: 1.0, 24: 1.0, 32: 1.0, 38: 1.0, 49: 1.0, 54: 1.0, 69: 1.0, 82: 1.0, 84: 1.0, 111: 1.0, 137: 1.0, 142: 1.0, 212: 1.0, 219: 1.0, 235: 1.0, 283: 1.0, 287: 1.0, 322: 1.0, 487: 1.0, 498: 1.0, 504: 1.0, 512: 1.0, 637: 1.0, 734: 1.0, 779: 1.0, 792: 1.0, 844: 1.0, 1129: 1.0, 1217: 1.0, 2045: 1.0, 2132: 1.0, 2311: 1.0, 2936: 1.0, 4690: 1.0, 11578: 1.0, 11632: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['such', 'a', 'charge', 'is', 'absurd', 'to', 'anyone', 'who', 'really', 'knows', 'him'], SparseVector(12847, {2: 1.0, 4: 1.0, 16: 1.0, 41: 1.0, 134: 1.0, 276: 1.0, 411: 1.0, 696: 1.0, 1126: 1.0, 4387: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['i', 'must', 'go', 'home', 'now', 'for', 'dad', 'is', 'very', 'ill', 'and', 'he', 'misses', 'me', 'so', 'if', 'i', 'leave', 'him'], SparseVector(12847, {1: 1.0, 6: 1.0, 11: 1.0, 16: 1.0, 22: 1.0, 32: 1.0, 34: 1.0, 49: 1.0, 57: 1.0, 84: 1.0, 104: 1.0, 152: 1.0, 270: 1.0, 402: 1.0, 1495: 1.0, 5458: 1.0, 10422: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['and', 'what', 'did', 'you', 'learn', 'from', 'him'], SparseVector(12847, {1: 1.0, 13: 1.0, 28: 1.0, 31: 1.0, 50: 1.0, 2432: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['this', 'business', 'has', 'had', 'a', 'very', 'bad', 'effect', 'upon', 'him'], SparseVector(12847, {4: 1.0, 17: 1.0, 29: 1.0, 83: 1.0, 84: 1.0, 131: 1.0, 412: 1.0, 587: 1.0, 833: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['in', 'a', 'hundred', 'other', 'ways', 'he', 'has', 'helped', 'him'], SparseVector(12847, {4: 1.0, 5: 1.0, 6: 1.0, 83: 1.0, 126: 1.0, 560: 1.0, 1859: 1.0, 2451: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['everybody', 'about', 'here', 'speaks', 'of', 'his', 'kindness', 'to', 'him'], SparseVector(12847, {2: 1.0, 3: 1.0, 14: 1.0, 69: 1.0, 97: 1.0, 580: 1.0, 3789: 1.0, 4789: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['men', 'who', 'had', 'only', 'known', 'the', 'quiet', 'thinker', 'and', 'logician', 'of', 'baker', 'street', 'would', 'have', 'failed', 'to', 'recognise', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 17: 1.0, 30: 1.0, 41: 1.0, 59: 1.0, 71: 1.0, 95: 1.0, 382: 1.0, 657: 1.0, 922: 1.0, 1480: 1.0, 2243: 1.0, 6586: 1.0, 10824: 1.0, 12664: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['well', 'now', 'in', 'considering', 'this', 'case', 'there', 'are', 'two', 'points', 'about', 'young', 'mccarthy', 's', 'narrative', 'which', 'struck', 'us', 'both', 'instantly', 'although', 'they', 'impressed', 'me', 'in', 'his', 'favour', 'and', 'you', 'against', 'him'], SparseVector(12847, {1: 1.0, 5: 1.0, 13: 1.0, 14: 1.0, 29: 1.0, 34: 1.0, 38: 1.0, 39: 1.0, 40: 1.0, 52: 1.0, 57: 1.0, 61: 1.0, 69: 1.0, 112: 1.0, 156: 1.0, 196: 1.0, 213: 1.0, 268: 1.0, 400: 1.0, 550: 1.0, 1015: 1.0, 1456: 1.0, 1661: 1.0, 2106: 1.0, 2682: 1.0, 2841: 1.0, 4312: 1.0, 5208: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['before', 'seeing', 'him'], SparseVector(12847, {67: 1.0, 363: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['holmes', 'i', 'said', 'you', 'have', 'drawn', 'a', 'net', 'round', 'this', 'man', 'from', 'which', 'he', 'cannot', 'escape', 'and', 'you', 'have', 'saved', 'an', 'innocent', 'human', 'life', 'as', 'truly', 'as', 'if', 'you', 'had', 'cut', 'the', 'cord', 'which', 'was', 'hanging', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 4: 1.0, 6: 1.0, 10: 1.0, 11: 1.0, 13: 1.0, 17: 1.0, 23: 1.0, 24: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 39: 1.0, 49: 1.0, 51: 1.0, 53: 1.0, 79: 1.0, 145: 1.0, 208: 1.0, 331: 1.0, 618: 1.0, 640: 1.0, 811: 1.0, 1201: 1.0, 1484: 1.0, 1549: 1.0, 1961: 1.0, 2400: 1.0, 3740: 1.0, 8682: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['holmes', 'rose', 'and', 'sat', 'down', 'at', 'the', 'table', 'with', 'his', 'pen', 'in', 'his', 'hand', 'and', 'a', 'bundle', 'of', 'paper', 'before', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 12: 1.0, 14: 1.0, 18: 1.0, 67: 1.0, 82: 1.0, 85: 1.0, 145: 1.0, 223: 1.0, 389: 1.0, 408: 1.0, 796: 1.0, 4489: 1.0, 12014: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['some', 'too', 'have', 'baffled', 'his', 'analytical', 'skill', 'and', 'would', 'be', 'as', 'narratives', 'beginnings', 'without', 'an', 'ending', 'while', 'others', 'have', 'been', 'but', 'partially', 'cleared', 'up', 'and', 'have', 'their', 'explanations', 'founded', 'rather', 'upon', 'conjecture', 'and', 'surmise', 'than', 'on', 'that', 'absolute', 'logical', 'proof', 'which', 'was', 'so', 'dear', 'to', 'him'], SparseVector(12847, {1: 1.0, 2: 1.0, 8: 1.0, 10: 1.0, 14: 1.0, 19: 1.0, 20: 1.0, 24: 1.0, 27: 1.0, 30: 1.0, 32: 1.0, 39: 1.0, 43: 1.0, 51: 1.0, 55: 1.0, 59: 1.0, 77: 1.0, 96: 1.0, 113: 1.0, 131: 1.0, 132: 1.0, 177: 1.0, 248: 1.0, 263: 1.0, 273: 1.0, 528: 1.0, 1295: 1.0, 1313: 1.0, 2087: 1.0, 2255: 1.0, 2497: 1.0, 3330: 1.0, 3811: 1.0, 3843: 1.0, 4420: 1.0, 6422: 1.0, 9481: 1.0, 10042: 1.0, 10278: 1.0, 11551: 1.0, 12606: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['i', 'signed', 'the', 'paper', 'as', 'directed', 'and', 'the', 'lawyer', 'took', 'it', 'away', 'with', 'him'], SparseVector(12847, {0: 1.0, 1: 1.0, 7: 1.0, 11: 1.0, 12: 1.0, 24: 1.0, 78: 1.0, 178: 1.0, 796: 1.0, 2090: 1.0, 3445: 1.0, 4702: 1.0}), SparseVector(12847, {9: 1.0}), 1),\n",
    " ('him', ['nigh', 'certain', 'that', 'some', 'foul', 'plot', 'had', 'been', 'woven', 'round', 'him'], SparseVector(12847, {8: 1.0, 17: 1.0, 55: 1.0, 96: 1.0, 208: 1.0, 483: 1.0, 4166: 1.0, 5266: 1.0, 6505: 1.0, 9295: 1.0}), SparseVector(12847, {9: 1.0}), 1)], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training:  31\n",
      "Number test:  19\n"
     ]
    }
   ],
   "source": [
    "# Split the examples into train and test, use 80/20 split\n",
    "df_trainset, df_testset = df_model.randomSplit((0.80, 0.20), 42)\n",
    "\n",
    "# Print the number of training examples\n",
    "print(\"Number training: \", df_trainset.count())\n",
    "\n",
    "# Print the number of test examples\n",
    "print(\"Number test: \", df_testset.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations:  0\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression classifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Instantiate logistic setting elasticnet to 0.0\n",
    "logistic = LogisticRegression(maxIter=10, regParam=0.4, elasticNetParam=0.0)\n",
    "\n",
    "# Train the logistic classifer on the trainset\n",
    "df_fitted = logistic.fit(df_trainset)\n",
    "\n",
    "# Print the number of training iterations\n",
    "print(\"Training iterations: \", df_fitted.summary.totalIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test AUC: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "testSummary = df_fitted.evaluate(df_testset)\n",
    "\n",
    "# Print the AUC metric\n",
    "print(\"\\ntest AUC: %.3f\" % testSummary.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "prediction : 1.0\n",
      "label : 1\n",
      "endword : him\n",
      "doc : ['has', 'only', 'one', 'male', 'visitor', 'but', 'a', 'good', 'deal', 'of', 'him']\n",
      "probability : [0.0,1.0]\n",
      "\n",
      "prediction : 1.0\n",
      "label : 1\n",
      "endword : him\n",
      "doc : ['he', 'was', 'pacing', 'the', 'room', 'swiftly', 'eagerly', 'with', 'his', 'head', 'sunk', 'upon', 'his', 'chest', 'and', 'his', 'hands', 'clasped', 'behind', 'him']\n",
      "probability : [0.0,1.0]\n",
      "\n",
      "prediction : 1.0\n",
      "label : 1\n",
      "endword : him\n",
      "doc : ['i', 'shall', 'drive', 'out', 'in', 'the', 'park', 'at', 'five', 'as', 'usual', 'she', 'said', 'as', 'she', 'left', 'him']\n",
      "probability : [0.0,1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nINCORRECT ==> \\nprediction : 0.0\\nlabel : 1\\nendword : him\\ndoc : ['bind', 'him', 'bind', 'him']\\nprobability : [0.5623411025382637,0.43765889746173625]\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = ['prediction', 'label', 'endword', 'doc', 'probability']\n",
    "\n",
    "# Apply the model to the test data\n",
    "predictions = df_fitted.transform(df_testset).select(fields)\n",
    "\n",
    "# Print incorrect if prediction does not match label\n",
    "for x in predictions.take(3):\n",
    "    print()\n",
    "    if x.label != int(x.prediction):\n",
    "        print(\"INCORRECT ==> \")\n",
    "    for y in fields:\n",
    "        print(y,\":\", x[y])    \n",
    "\n",
    "'''\n",
    "INCORRECT ==> \n",
    "prediction : 0.0\n",
    "label : 1\n",
    "endword : him\n",
    "doc : ['bind', 'him', 'bind', 'him']\n",
    "probability : [0.5623411025382637,0.43765889746173625]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
